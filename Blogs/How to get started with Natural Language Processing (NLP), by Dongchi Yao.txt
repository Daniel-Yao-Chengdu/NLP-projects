Intro
I chatted with many friends recently, and some of them are optimistic about the future development of NLP: how to make the model better understand industry-specific knowledge, and how to integrate voice and image into NLP for multi-modal simulation. During those chats, I also learned that some undergraduate students want to enter the NLP field because they think that the competition is fiercer in the CV field than in the NLP field. However, I still think that it is mainly based on your personal interests, and try to do whatever you like, which is very important for your career development. This article will talk about how to learn NLP from scratch.

Assumptions
First, I assume you are familiar with the basic concepts of probability and statistics, linear algebra, and machine learning. If you donâ€™t understand these things, then go to study them systematically first.

The basic modules of NLP
CNN structure, RNN structure, GRU, LSTM, and transformer. These basic structures are very important, just like the skeleton framework supporting the development of the entire field. If you are short on time, you can start to study directly from the transformer. Now the excellent works are almost all based on the transformer model.

NLP learning process
--Word2Vec, GloVe, fastText in and after 2013
These are very, very important. The authors vectorized the words through neural networks and generated the word embeddings. The concept of word embedding is really amazing, which converts each word/token into numbers, after which the computer can recognize and process it. Word embedding will accompany you throughout the entire NLP learning path, and the SOTA models are all studying how to make word embeddings more meaningful and more task-specific.

 --Play with the pre-trained word embeddings
Utilize the pre-trained word embeddings generated by the above models and send them to sequence models such as CNN(sometimes), RNN, LSTM, etc. to process sentences and articles, and do some tasks like text classification.

--The Seq2Seq framework around 2015
This framework is of great significance to the field of machine translation using neural networks and is a classic encoder-decoder model.

--The ELMo model in 2018
The word embeddings generated by forward propagation and backward propagation are concatenated together so that the integrated embedding can be considered to have bidirectional semantics, but the effect is not very good compared with later transformer-based models.

--The transformer framework in 2017 (Attention is all you need)
God, this framework is really very important, which fully utilized the self-attention mechanism to learn context. It can be said that it has directly changed the direction of the NLP field, and is now gradually changing the CV field. If you don't learn transformer well, don't enter NLP!

--ULMFIT in 2018
This model is the pioneer of transfer learning in the field of NLP. The authors pre-trained the LSTM model and then used the word embeddings generated to do downstream tasks.

--Try to fully master the concept of transfer learning, you must understand it!

--GPT and BERT in 2018
These two models fully signified that the NLP field entered the era of pre-training large models (transfer learning). Well, in 2018, the two models of GPT and BERT can be said to have established a new era of NLP. The models after 2018 are based on GPT and BERT, of which many people said that BERT has a higher privilege. Afterward, the T5 model is released, which is a typical Seq2Seq model, and unified all tasks into a text-to-text format.

--For new models after 2018, you can read the papers for a deep understanding. This article lists some of the models as follows: (The classification is rough because there are overlaps among some models)
BERT like: TinyBERT, RoBERTa, XLM, ALBERT, ELECTRA, StructBERT, DeBERTa, SpanBERT, ERNIE, etc.
GPT like: GPT1,2 and 3, XLNet, Transformer-XL, CTRL, GLAM, etc.
T5 like: UniLM, MASS, BART, GLM, etc.

Suggestion
While learning, you should do some coding work to help you better understand the concepts and models. The tasks that I suggest you do are text classification, text generation, NER. After completing these basic tasks, you can do some more difficult tasks, such as summarization and dialogue.