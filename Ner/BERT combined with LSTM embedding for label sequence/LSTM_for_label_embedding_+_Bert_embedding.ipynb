{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM for label embedding + Bert embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import"
      ],
      "metadata": {
        "id": "I3_4pOFtsePJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJbzcfndByKF"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-crf\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torchcrf import CRF\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "xDnGdWCN-XNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for reading original ids and labels\n",
        "def read_original(x):\n",
        "  data = load_dataset('conll2003', split=x)\n",
        "  text=[' '.join(i) for i in data[0:-1]['tokens']]\n",
        "  id=[tokenizer(i, return_tensors=\"pt\")['input_ids'] for i in text]\n",
        "  labels=[torch.tensor(i) for i in data[0:-1]['ner_tags']]\n",
        "  return data, text, id, labels\n",
        "\n",
        "train_data,train_text,train_data_id,train_labels=read_original('train')\n",
        "val_data,val_text,val_data_id,val_labels=read_original('validation')\n",
        "test_data,test_text,test_data_id,test_labels=read_original('test')"
      ],
      "metadata": {
        "id": "kd5jLbzdB0pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create paddings for bert tokenizer\n",
        "def create_padding(x):\n",
        "  paddings=[]\n",
        "  for i in x:\n",
        "    sentence=i.split(' ')\n",
        "    m=[tokenizer.encode(w) for w in sentence]\n",
        "    padding=[]\n",
        "    for j in m:\n",
        "      if len(j)==3: padding.append([1])\n",
        "      else: \n",
        "        new_padding=(len(j)-2)*[0]\n",
        "        new_padding[0]=1\n",
        "        padding.append(new_padding)\n",
        "    new_padding=[]\n",
        "    for k in padding:\n",
        "      new_padding+=k\n",
        "    new_padding.insert(0,0)\n",
        "    new_padding=new_padding+[0]\n",
        "    paddings.append(new_padding)\n",
        "  return paddings\n",
        "\n",
        "train_padding=create_padding(train_text)\n",
        "val_padding=create_padding(val_text)\n",
        "test_padding=create_padding(test_text)"
      ],
      "metadata": {
        "id": "pUY33Au_B6Im"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make the length of the label equal to the bert_padding by adding label -1\n",
        "#get the new labels\n",
        "def get_new_labels(labels,padding):\n",
        "  new_labels=[]\n",
        "  for j in range (len(labels)):\n",
        "    N=-1\n",
        "    new_label=[]\n",
        "    for i in padding[j]:\n",
        "      if i==0:new_label.append(-1)\n",
        "      if i!=0:N+=1; new_label.append(labels[j][N].item())\n",
        "    new_labels.append(torch.tensor(new_label))\n",
        "  return new_labels\n",
        "\n",
        "new_train_labels=get_new_labels(train_labels,train_padding)\n",
        "new_val_labels=get_new_labels(val_labels,val_padding)\n",
        "new_test_labels=get_new_labels(test_labels,test_padding)"
      ],
      "metadata": {
        "id": "NYdqm5JpB78U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[seq][seq][seq]\n",
        "train_data=[[i.squeeze(0),j,k]   for i, j, k   in zip(train_data_id, new_train_labels, train_labels)]\n",
        "val_data  =[[i.squeeze(0),j,k]   for i, j, k   in zip(val_data_id, new_val_labels, val_labels)]\n",
        "test_data =[[i.squeeze(0),j,k]   for i, j, k   in zip(test_data_id, new_test_labels, test_labels)]"
      ],
      "metadata": {
        "id": "Blct6voSB-qX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM model for obtaining label embedding\n",
        "* Autoencoder idea: \"you are\"--two 768 embeddings--LSTM--two 768 embeddings--predict--compare with \"you are\" again. We can also use Language Modelling idea to predict the next word. \n",
        "* Teacher forcing idea "
      ],
      "metadata": {
        "id": "GQj_4vr5CIsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(torch.nn.Module):\n",
        "    def __init__(self, hidden_size): #set parameters \n",
        "        super().__init__()\n",
        "        self.embedding=torch.nn.Embedding(9,768)\n",
        "        self.LSTM = torch.nn.LSTM(input_size=768, hidden_size=768, batch_first=True) \n",
        "        self.linear = torch.nn.Linear(in_features=768, out_features=9)\n",
        "\n",
        "    def forward(self, x): #m,seq\n",
        "        x=self.embedding(x) #m,seq,768\n",
        "        x,(hidden_state,cell_state) = self.LSTM(x) # m,seq,768\n",
        "        logits = self.linear(x)  # logitsï¼š(batch size, seq_length, 9)\n",
        "        return logits\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(0)\n",
        "model1=LSTM(768).to(device)"
      ],
      "metadata": {
        "id": "9A2ZFPdJCHhx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(val_dataloader, model):\n",
        "      model.eval()\n",
        "      accuracy=0\n",
        "      total=0\n",
        "      for i, batch in enumerate(val_dataloader):\n",
        "        x=batch[0].to(device).long()#m,seq\n",
        "        y=batch[0].to(device)\n",
        "        with torch.no_grad():\n",
        "          x=model(x) #m,seq,9\n",
        "        x=torch.softmax(x,-1) #m,seq,9\n",
        "        x=torch.argmax(x,-1)#m,seq\n",
        "        preds=x.view(-1).to('cpu') #m*seq\n",
        "        y=y.view(-1).to('cpu') #m*seq\n",
        "        new_label=[]\n",
        "        new_pred=[]\n",
        "        for j in range (y.shape[-1]):\n",
        "          if y[j]!=-1:\n",
        "            new_label.append(y[j])\n",
        "            new_pred.append(preds[j])\n",
        "        accuracy+=accuracy_score(new_label,new_pred)*len(new_pred)\n",
        "        total+=len(new_pred)\n",
        "      return accuracy/total"
      ],
      "metadata": {
        "id": "u7Z6k-nuA-S3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer, num_batch, val_dataloader, len_val, criterion):\n",
        "    accumulating_batch_count = 0\n",
        "    for epoch in range(epochs):\n",
        "      print(f\"Training epoch {epoch+1}\")\n",
        "      model.train()\n",
        "      for i, batch in enumerate(train_dataloader):\n",
        "          x=batch[0].to(device)#seq\n",
        "          logits=model(x)#seq,9\n",
        "          loss=criterion(logits,x)/batchsize_grad\n",
        "          loss.backward() \n",
        "          if accumulating_batch_count % batchsize_grad == 0: \n",
        "              optimizer.step()\n",
        "              #scheduler.step()\n",
        "              optimizer.zero_grad()\n",
        "              model.zero_grad()\n",
        "              print (i+1,'loss',loss.item())\n",
        "          accumulating_batch_count += 1\n",
        "      print ('test accuracy', test(val_dataloader, model))"
      ],
      "metadata": {
        "id": "DKwEEL0xDKt0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=train_labels[0:200]\n",
        "batch_size=1\n",
        "torch.manual_seed(0)\n",
        "train_dataloader = DataLoader(input, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "input=test_labels[0:200]\n",
        "torch.manual_seed(0)\n",
        "val_dataloader = DataLoader(input, batch_size=1, shuffle=True)\n",
        "len_val=len(val_dataloader)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "batchsize_grad=4\n",
        "epochs=5 #simple model uses more epochs\n",
        "lr=0.03 #simple models uses larger lr\n",
        "num_batch=round(len(train_data)/batch_size)-1\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=epochs*(len(train_dataloader)/batchsize_grad))\n",
        "train(train_dataloader,model1,batchsize_grad,epochs,scheduler,optimizer, num_batch,val_dataloader, len_val,criterion)"
      ],
      "metadata": {
        "id": "fkik_wX6GTg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct a model that combines LSTM embedding and bert embedding for prediction\n",
        "* LSTM embedding of previous labels + Bert embedding of current text to predict the current label\n",
        "* If it is the first word in the sentence, the LSTM embedding is zeros because no previous label exists."
      ],
      "metadata": {
        "id": "FIwTxrnJM-1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(torch.nn.Module):\n",
        "    def __init__(self, model,bertmodel): #set parameters \n",
        "        super().__init__()\n",
        "        self.bert=bertmodel\n",
        "        self.embed=model.embedding\n",
        "        self.lstm=model.LSTM\n",
        "        self.linear = torch.nn.Linear(in_features=768*2, out_features=9)\n",
        "\n",
        "    def forward(self, x,y,z): #m,seq; m,seq\n",
        "        x=self.bert(x)[0] #1,seq,768\n",
        "        new_x=torch.zeros([1,768])\n",
        "\n",
        "        #for remove the embeddings of -1 position\n",
        "        for i in range (y.shape[1]):\n",
        "          if y[0][i]!=-1:new_x=torch.cat((new_x,x[0][i].unsqueeze(0)),0)\n",
        "        x=new_x[1:] #seq,768\n",
        "\n",
        "        seq=self.embed(z.squeeze(0)) #seq,768\n",
        "        seq=self.lstm(seq)[0] #seq,768\n",
        "        seq=torch.cat((torch.zeros([1,768]),seq[:-1]),0)#seq,768\n",
        "\n",
        "        x=torch.cat((x,seq),-1) #seq,768*2\n",
        "        logits = self.linear(x)  #seq,9\n",
        "        return logits\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bertmodel=BertModel.from_pretrained('bert-base-cased').to(device)\n",
        "for param in bertmodel.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model1.parameters():\n",
        "    param.requires_grad = False\n",
        "torch.manual_seed(0)\n",
        "model2=BERT(model1,bertmodel).to(device)"
      ],
      "metadata": {
        "id": "giwJlDVKIpAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def teacher_test(val_dataloader, model):\n",
        "      model.eval()\n",
        "      accuracy=0\n",
        "      total=0\n",
        "      for i, batch in enumerate(val_dataloader):\n",
        "        x=batch[0].to(device)#m,seq\n",
        "        y=batch[1].to(device)\n",
        "        z=batch[2].to(device) # seq        \n",
        "        with torch.no_grad():\n",
        "          logits=model(x,y,z) #m,seq,9\n",
        "        logits=torch.softmax(logits,-1) #m,seq,9\n",
        "        preds=torch.argmax(logits,-1)#m,seq\n",
        "        preds=preds.view(-1).to('cpu') #m*seq\n",
        "        z=z.view(-1).to('cpu') #m*seq\n",
        "        #print (preds)\n",
        "        #print (z)\n",
        "        accuracy+=accuracy_score(z,preds)*len(preds)\n",
        "        total+=len(preds)\n",
        "      return round(accuracy/total,3)"
      ],
      "metadata": {
        "id": "AVrsv35GDrcf"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer, num_batch, val_dataloader, len_val, criterion):\n",
        "\n",
        "    accumulating_batch_count = 0\n",
        "    for epoch in range(epochs):\n",
        "      print(f\"Training epoch {epoch+1}\")\n",
        "      model.train()\n",
        "      for i, batch in enumerate(train_dataloader):\n",
        "          x=batch[0].to(device)#m,seq\n",
        "          y=batch[1].to(device)\n",
        "          z=batch[2].to(device) # seq\n",
        "          logits=model(x,y,z).view(-1,9) #seq,9\n",
        "          loss=criterion(logits,z.view(-1))/batchsize_grad\n",
        "          loss.backward() #The gradients are computed when we call loss. backward() and are stored by PyTorch until we call optimizer.\n",
        "          if accumulating_batch_count % batchsize_grad == 0: #when accumulated batch=16, we do optimizer after 16 batches of gradients are accumulated\n",
        "              optimizer.step()\n",
        "              #scheduler.step()\n",
        "              optimizer.zero_grad()\n",
        "              model.zero_grad()\n",
        "              #print (i+1,'loss',loss.item())\n",
        "          accumulating_batch_count += 1\n",
        "      print ('teacher_test',teacher_test(val_dataloader,model))\n",
        "      #model2=model\n",
        "      #save the best model\n",
        "      #if accuracy/len_val>0.82: path=\"best_model.pt\"; torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "zpuIUgOrXlP5"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameter\n",
        "batch_size=1\n",
        "batchsize_grad=2\n",
        "epochs=4 #simple model uses more epochs\n",
        "lr=0.00005 #simple models uses larger lr\n",
        "\n",
        "#create training data\n",
        "train_input=[[i[0].squeeze(0),i[1].squeeze(0),j] for i, j in zip(train_data, train_labels)]\n",
        "train_dataloader = DataLoader(train_input[0:100], batch_size=batch_size, shuffle=True, worker_init_fn=torch.manual_seed(0))\n",
        "\n",
        "#create val data\n",
        "val_dataloader = DataLoader(train_input[0:100], batch_size=1, shuffle=True,worker_init_fn=torch.manual_seed(0))\n",
        "len_val=len(val_dataloader)\n",
        "\n",
        "#configuration\n",
        "num_batch = round(len(train_data)/batch_size)-1\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model2.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=epochs*(len(train_dataloader)/batchsize_grad))"
      ],
      "metadata": {
        "id": "mnbqNrEkVIMF"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataloader,model2,batchsize_grad,2,scheduler,optimizer, num_batch,val_dataloader, len_val, criterion)"
      ],
      "metadata": {
        "id": "-aOgyaW4bdcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model\n",
        "* Sampling step by step instead of teacher forcing idea for testing"
      ],
      "metadata": {
        "id": "ZMxc6G-JJ6vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling_test(val_dataloader, model):\n",
        "      model.eval()\n",
        "      accuracy=0\n",
        "      total=0\n",
        "      for i, batch in enumerate(val_dataloader):\n",
        "        x=batch[0].to(device)#m, seq\n",
        "        y=batch[1].to(device)#m, seq\n",
        "        z=batch[2].to(device)#m, seq\n",
        "\n",
        "        with torch.no_grad():\n",
        "          x=model.bert(x)[0] #m,seq,768\n",
        "        new_x=torch.zeros([1,768])\n",
        "        #for remove the embeddings of -1 position\n",
        "        for i in range (x.shape[1]):\n",
        "          if y[0][i]!=-1:new_x=torch.cat((new_x,x[0][i].unsqueeze(0)),0)\n",
        "        x=new_x[1:] #seq2,768\n",
        "\n",
        "        #start to iterate over each position\n",
        "        predicts=[]                  \n",
        "        for j in range (x.shape[0]):\n",
        "          if j==0: \n",
        "            lstm_embedding=torch.zeros([768])#768\n",
        "            new_x=torch.cat((x[j],lstm_embedding),0) #768*2\n",
        "            logits=model.linear(new_x) #9\n",
        "            logits=torch.softmax(logits,-1)#9\n",
        "            predict=torch.argmax(logits,-1)#1\n",
        "            predicts.append(predict)\n",
        "\n",
        "          else: \n",
        "            lstm_embedding=model.embed(torch.tensor(predicts))\n",
        "            lstm_embedding=model.lstm(lstm_embedding)[0] #seq3,768\n",
        "            lstm_embedding=lstm_embedding[-1]\n",
        "            new_x=torch.cat((x[j],lstm_embedding),0)\n",
        "            logits=model.linear(new_x)\n",
        "            logits=torch.softmax(logits,-1)\n",
        "            predict=torch.argmax(logits,-1)\n",
        "            predicts.append(predict)\n",
        "        predicts=torch.tensor(predicts)\n",
        "        z=z.squeeze(0)\n",
        "        #print (predicts)\n",
        "        #print (z.squeeze(0))\n",
        "        accuracy+=accuracy_score(z,predicts)*len(predicts)\n",
        "        total+=len(predicts)\n",
        "      return round(accuracy/total,3)"
      ],
      "metadata": {
        "id": "BwbeorUuNUCT"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input=[[i[0].squeeze(0),i[1].squeeze(0),j] for i, j in zip(train_data, train_labels)]\n",
        "test_input=[[i[0].squeeze(0),i[1].squeeze(0),j] for i, j in zip(test_data, test_labels)]\n",
        "test_dataloader = DataLoader(test_input[0:100], batch_size=1, shuffle=True,worker_init_fn=torch.manual_seed(0))\n",
        "sampling_test(test_dataloader, model2)"
      ],
      "metadata": {
        "id": "V_MlfzIddguA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}