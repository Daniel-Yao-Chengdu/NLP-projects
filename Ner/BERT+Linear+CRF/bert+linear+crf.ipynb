{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert+linear+crf.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SBa9YZG66Q-I"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reference, install and import\n",
        "* bilstm_crf: [here](https://pypi.org/project/bi-lstm-crf/)\n",
        "* Before BERT, LSTM+CRF \n",
        "* After BERT, Bert+Linear (BertForTokenClassification) is mostly enough\n",
        "* BERT+LSTM+CRF seems not that powerful, do not add LSTM layer!\n",
        "* Sometimes, BERT+CRF or BERT+LSTM+CRF is ok\n",
        "* A detailed comparison and discussion is [here](https://posts.careerengine.us/p/60763f6009edcc27e3dfd854)\n"
      ],
      "metadata": {
        "id": "SBa9YZG66Q-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vagLsUJiYXAD"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-crf\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torchcrf import CRF\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for reading original ids and labels\n",
        "def read_original(x):\n",
        "  data = load_dataset('conll2003', split=x)\n",
        "  text=[' '.join(i) for i in data[0:-1]['tokens']]\n",
        "  id=[tokenizer(i, return_tensors=\"pt\")['input_ids'] for i in text]\n",
        "  labels=[torch.tensor(i) for i in data[0:-1]['ner_tags']]\n",
        "  return data, text, id, labels\n",
        "\n",
        "train_data,train_text,train_data_id,train_labels=read_original('train')\n",
        "val_data,val_text,val_data_id,val_labels=read_original('validation')\n",
        "test_data,test_text,test_data_id,test_labels=read_original('test')"
      ],
      "metadata": {
        "id": "Ok6MAJ3SjMSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca52120-e44b-4b38-d1f5-b974bc21fc74"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create paddings for bert tokenizer\n",
        "def create_padding(x):\n",
        "  paddings=[]\n",
        "  for i in x:\n",
        "    sentence=i.split(' ')\n",
        "    m=[tokenizer.encode(w) for w in sentence]\n",
        "    padding=[]\n",
        "    for j in m:\n",
        "      if len(j)==3: padding.append([1])\n",
        "      else: \n",
        "        new_padding=(len(j)-2)*[0]\n",
        "        new_padding[0]=1\n",
        "        padding.append(new_padding)\n",
        "    new_padding=[]\n",
        "    for k in padding:\n",
        "      new_padding+=k\n",
        "    new_padding.insert(0,0)\n",
        "    new_padding=new_padding+[0]\n",
        "    paddings.append(new_padding)\n",
        "  return paddings\n",
        "\n",
        "train_padding=create_padding(train_text)\n",
        "val_padding=create_padding(val_text)\n",
        "test_padding=create_padding(test_text)"
      ],
      "metadata": {
        "id": "mNC2SGi0lOuW"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make the length of the label equal to the bert_padding by adding label -1\n",
        "#get the new labels\n",
        "def get_new_labels(labels,padding):\n",
        "  new_labels=[]\n",
        "  for j in range (len(labels)):\n",
        "    N=-1\n",
        "    new_label=[]\n",
        "    for i in padding[j]:\n",
        "      if i==0:new_label.append(-1)\n",
        "      if i!=0:N+=1; new_label.append(labels[j][N].item())\n",
        "    new_labels.append(torch.tensor(new_label))\n",
        "  return new_labels\n",
        "\n",
        "new_train_labels=get_new_labels(train_labels,train_padding)\n",
        "new_val_labels=get_new_labels(val_labels,val_padding)\n",
        "new_test_labels=get_new_labels(test_labels,test_padding)"
      ],
      "metadata": {
        "id": "EKt7WVJIm8Hl"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to now, the useful data is [data_id, new_labels], which have the same lengths. "
      ],
      "metadata": {
        "id": "TuAD69fGoy8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#[m,seq][m,seq]\n",
        "train_data=[[i,j.unsqueeze(0),torch.tensor(k).unsqueeze(0).bool()]   for i, j, k   in zip(train_data_id, new_train_labels, train_padding)]\n",
        "val_data  =[[i,j.unsqueeze(0),torch.tensor(k).unsqueeze(0).bool()]   for i, j, k   in zip(val_data_id, new_val_labels, val_padding)]\n",
        "test_data =[[i,j.unsqueeze(0),torch.tensor(k).unsqueeze(0).bool()]   for i, j, k   in zip(test_data_id, new_test_labels, test_padding)]"
      ],
      "metadata": {
        "id": "YNYpvudw5uLN"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "qQ590ytH--Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertCRF(torch.nn.Module):\n",
        "    def __init__(self, bertmodel, CRFmodel): \n",
        "        super().__init__()\n",
        "        self.bertmodel=bertmodel\n",
        "        self.linear=torch.nn.Linear(768,9,bias=True)\n",
        "        self.crf=CRFmodel\n",
        "\n",
        "    def forward(self, x,y,z): #[m,seq],[m,seq]\n",
        "        x=self.bertmodel(x)[0] #m,seq,768 \n",
        "        x=self.linear(x)#m,seq,9\n",
        "        loss=-self.crf.forward(x,y,z,reduction='token_mean')\n",
        "        return loss\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bertmodel=BertModel.from_pretrained('bert-base-cased').to(device)\n",
        "for param in bertmodel.parameters():\n",
        "    param.requires_grad = False\n",
        "torch.manual_seed(0)\n",
        "CRFmodel = CRF(9, batch_first=True).to(device)\n",
        "torch.manual_seed(0)\n",
        "model=BertCRF(bertmodel, CRFmodel).to(device)"
      ],
      "metadata": {
        "id": "4w3451hV3yUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb14c62d-0e7e-48a8-c004-89ca6680c359"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer, num_batch, val_dataloader, len_val):\n",
        "\n",
        "    accumulating_batch_count = 0\n",
        "    for epoch in range(epochs):\n",
        "      print(f\"Training epoch {epoch+1}\")\n",
        "      model.train()\n",
        "      for i, batch in enumerate(train_dataloader):\n",
        "          x=batch[0].squeeze(1)[:,1:].to(device).long()#m,seq\n",
        "          y=batch[1].squeeze(1)[:,1:].to(device)\n",
        "          z=batch[2].squeeze(1)[:,1:].to(device)\n",
        "          loss=model(x,y,z)/batchsize_grad\n",
        "          loss.backward() #The gradients are computed when we call loss. backward() and are stored by PyTorch until we call optimizer.\n",
        "\n",
        "          if accumulating_batch_count % batchsize_grad == 0: #when accumulated batch=16, we do optimizer after 16 batches of gradients are accumulated\n",
        "              optimizer.step()\n",
        "              #scheduler.step()\n",
        "              optimizer.zero_grad()\n",
        "              model.zero_grad()\n",
        "              #print (i+1,'loss',loss.item())\n",
        "          accumulating_batch_count += 1\n",
        "      \n",
        "      #eval the model\n",
        "      model.eval()\n",
        "      accuracy=0\n",
        "      total=0\n",
        "      for i, batch in enumerate(val_dataloader):\n",
        "        x=batch[0].squeeze(1)[:,1:].to(device).long()#m,seq\n",
        "        y=batch[1].squeeze(1)[:,1:].to(device)#m,seq\n",
        "        z=batch[2].squeeze(1)[:,1:].to(device)\n",
        "        with torch.no_grad():\n",
        "          x=model.bertmodel(x)[0] #m,seq,768\n",
        "          x=model.linear(x)#m,seq,9\n",
        "          x=model.crf.decode(x,mask=z)#m,seq\n",
        "          x=torch.tensor(x)\n",
        "        predict_label=x.view(-1).to('cpu')\n",
        "        targets=y.view(-1).to('cpu') #m\n",
        "        targets=targets[targets!=-1]\n",
        "        accuracy+=accuracy_score(targets,predict_label)*x.shape[1]\n",
        "        total+=x.shape[1]\n",
        "      print (accuracy/total)\n",
        "      #save the best model\n",
        "      #if accuracy/len_val>0.82: path=\"best_model.pt\"; torch.save(model.state_dict(), path) "
      ],
      "metadata": {
        "id": "sjASvnOJAsCg"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create training data and val data\n",
        "input=train_data[0:100]\n",
        "batch_size=1\n",
        "torch.manual_seed(0)\n",
        "train_dataloader = DataLoader(input, batch_size=batch_size, shuffle=True)\n",
        "torch.manual_seed(0)\n",
        "val_dataloader = DataLoader(input, batch_size=1, shuffle=True)\n",
        "len_val=len(val_dataloader)\n",
        "\n",
        "#hyperparameter\n",
        "batchsize_grad=1\n",
        "epochs=4 #simple model uses more epochs\n",
        "lr=0.08 #simple models uses larger lr\n",
        "num_batch=round(len(train_data)/batch_size)-1\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=epochs*(len(train_dataloader)/batchsize_grad))\n",
        "scheduler=None\n",
        "\n",
        "train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer, num_batch,val_dataloader, len_val)"
      ],
      "metadata": {
        "id": "aoKMoYYFAsOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "py1QhbBDZgpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"drive/MyDrive/0414.pt\"\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "5NiJaMatTOTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=test_data\n",
        "val_dataloader = DataLoader(input[0:100], batch_size=1, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "accuracy=0\n",
        "total=0\n",
        "for i, batch in enumerate(val_dataloader):\n",
        "  x=batch[0].squeeze(1)[:,1:].to(device).long()#m,seq\n",
        "  y=batch[1].squeeze(1)[:,1:].to(device)#m,seq\n",
        "  z=batch[2].squeeze(1)[:,1:].to(device)\n",
        "  with torch.no_grad():\n",
        "    x=model.bertmodel(x)[0] #m,seq,768\n",
        "    x=model.linear(x)#m,seq,9\n",
        "    x=model.crf.decode(x,mask=z)#m,seq\n",
        "    x=torch.tensor(x)\n",
        "  predict_label=x.view(-1).to('cpu')\n",
        "  targets=y.view(-1).to('cpu') #m\n",
        "  targets=targets[targets!=-1]\n",
        "  accuracy+=accuracy_score(targets,predict_label)*x.shape[1]\n",
        "  total+=x.shape[1]\n",
        "print (accuracy/total)"
      ],
      "metadata": {
        "id": "6CT8uLRjZfBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D7G_OoQMxTCZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}