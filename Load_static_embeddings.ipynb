{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Load embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMg8v6AqgOtYpvccoL+JCxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniel-Yao-Chengdu/NLP-project/blob/master/Load_static_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained vectors here\n",
        "* https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html\n",
        "* we can download different vectors from the link above"
      ],
      "metadata": {
        "id": "VO2jFFYq3PG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fasttext embedding"
      ],
      "metadata": {
        "id": "95H97evip9fH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmHHEvPNpl8K"
      },
      "outputs": [],
      "source": [
        "# download fasttext pretrained embeddings\n",
        "import os\n",
        "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
        "FILE = \"fastText\"\n",
        "\n",
        "if os.path.isdir(FILE):\n",
        "    print(\"fastText exists.\")\n",
        "else:\n",
        "    !wget -P $FILE $URL\n",
        "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our own vocab\n",
        "!pip install NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentence = \"I love you, but also love NLP incredibly\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "vocab=set(tokens)\n",
        "vocab=list(vocab)"
      ],
      "metadata": {
        "id": "p-249GqTqzID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dictionary of embeddings. Here we only create the embedding dic that appear in our own vocab. \n",
        "#We can also create an embedding dict that involves all the original words.\n",
        "import torch\n",
        "fname=\"fastText/crawl-300d-2M.vec\"\n",
        "fin=open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "n, d = map(int, fin.readline().split())\n",
        "embedding_dic={}\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "for line in tqdm_notebook(fin):\n",
        "  tokens=line.rstrip().split(' ')\n",
        "  if tokens[0] in vocab: \n",
        "    embedding_dic[tokens[0]]=torch.tensor(list(map(float, tokens[1:]))).unsqueeze(0)"
      ],
      "metadata": {
        "id": "rR30L_IDqBEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dic"
      ],
      "metadata": {
        "id": "WBZHZ9VMtKCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec"
      ],
      "metadata": {
        "id": "ls9DTjQBuWEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!we need to download the GoogleNews-vectors-negative300.bin file from the website above, and then upload it into colab. \n",
        "!gunzip GoogleNews-vectors-negative300.bin"
      ],
      "metadata": {
        "id": "kdtuyrdlw_jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format ('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "# if you vector file is in binary format, change to binary=True\n",
        "sentence = [\"London\", \"is\", \"the\", \"capital\", \"of\", \"Great\", \"Britain\"]\n",
        "vectors = [model[w] for w in sentence]"
      ],
      "metadata": {
        "id": "H4WV_M3xuXy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glove\n",
        "* This zip file contains 4 files for 4 embedding representations.\n",
        "* After unzipping the downloaded file we find four txt files: glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, and glove.6B.300d.txt. As their filenames suggests, they have vectors with different dimensions."
      ],
      "metadata": {
        "id": "Yjhh_vkO-n_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "RusZ53IR-mjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "embedding_dic = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embedding_dic[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "0D8e-AAV-tNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dic['love']"
      ],
      "metadata": {
        "id": "gMna45to_1Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CPTBNFxQAZlg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}