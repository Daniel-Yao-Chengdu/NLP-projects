{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Load embeddings.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyMg8v6AqgOtYpvccoL+JCxh",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Daniel-Yao-Chengdu/NLP-project/blob/master/Load_static_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pretrained vectors here\n",
    "* https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html\n",
    "* we can download different vectors from the link above"
   ],
   "metadata": {
    "id": "VO2jFFYq3PG_",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fasttext embedding"
   ],
   "metadata": {
    "id": "95H97evip9fH",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "MmHHEvPNpl8K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n",
      "'unzip' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "# download fasttext pretrained embeddings\n",
    "import os\n",
    "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
    "FILE = \"fastText\"\n",
    "\n",
    "if os.path.isdir(FILE):\n",
    "    print(\"fastText exists.\")\n",
    "else:\n",
    "    !wget -P $FILE $URL\n",
    "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create our own vocab\n",
    "!pip install NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentence = \"I love you, but also love NLP incredibly\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "vocab=set(tokens)\n",
    "vocab=list(vocab)"
   ],
   "metadata": {
    "id": "p-249GqTqzID",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in c:\\users\\smrya\\appdata\\roaming\\python\\python38\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\smrya\\anaconda3\\lib\\site-packages (from NLTK) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\smrya\\anaconda3\\lib\\site-packages (from NLTK) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\smrya\\anaconda3\\lib\\site-packages (from NLTK) (2022.6.2)\n",
      "Requirement already satisfied: click in c:\\users\\smrya\\anaconda3\\lib\\site-packages (from NLTK) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\smrya\\anaconda3\\lib\\site-packages (from click->NLTK) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smrya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error downloading 'punkt' from\n",
      "[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-\n",
      "[nltk_data]     pages/packages/tokenizers/punkt.zip>:   <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrya\\anaconda3\\lib\\site-packages)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smrya\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in c:\\users\\smrya\\appdata\\roaming\\python\\python38\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\smrya\\anaconda3\\lib\\site-packages (from NLTK) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\smrya\\anaconda3\\lib\\site-packages (from NLTK) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\smrya\\anaconda3\\lib\\site-packages (from NLTK) (2022.6.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\smrya\\anaconda3\\lib\\site-packages (from NLTK) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\smrya\\anaconda3\\lib\\site-packages (from click->NLTK) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Create a dictionary of embeddings. Here we only create the embedding dic that appear in our own vocab. \n",
    "#We can also create an embedding dict that involves all the original words.\n",
    "import torch\n",
    "fname=\"fastText/crawl-300d-2M.vec\"\n",
    "fin=open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "n, d = map(int, fin.readline().split())\n",
    "embedding_dic={}\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "for line in tqdm_notebook(fin):\n",
    "  tokens=line.rstrip().split(' ')\n",
    "  if tokens[0] in vocab: \n",
    "    embedding_dic[tokens[0]]=torch.tensor(list(map(float, tokens[1:]))).unsqueeze(0)"
   ],
   "metadata": {
    "id": "rR30L_IDqBEG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_dic"
   ],
   "metadata": {
    "id": "WBZHZ9VMtKCF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2vec"
   ],
   "metadata": {
    "id": "ls9DTjQBuWEC",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!we need to download the GoogleNews-vectors-negative300.bin file from the website above, and then upload it into colab. \n",
    "!gunzip GoogleNews-vectors-negative300.bin"
   ],
   "metadata": {
    "id": "kdtuyrdlw_jw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format ('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# if you vector file is in binary format, change to binary=True\n",
    "sentence = [\"London\", \"is\", \"the\", \"capital\", \"of\", \"Great\", \"Britain\"]\n",
    "vectors = [model[w] for w in sentence]"
   ],
   "metadata": {
    "id": "H4WV_M3xuXy2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Glove\n",
    "* This zip file contains 4 files for 4 embedding representations.\n",
    "* After unzipping the downloaded file we find four txt files: glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, and glove.6B.300d.txt. As their filenames suggests, they have vectors with different dimensions."
   ],
   "metadata": {
    "id": "Yjhh_vkO-n_w",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip"
   ],
   "metadata": {
    "id": "RusZ53IR-mjW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "embedding_dic = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_dic[word] = coefs\n",
    "f.close()"
   ],
   "metadata": {
    "id": "0D8e-AAV-tNo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_dic['love']"
   ],
   "metadata": {
    "id": "gMna45to_1Sr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load pretrained vectors to a torch nn.Embedding\n",
    "* refer to the NLP course HW1 for detailed explanation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# read vectors from Glove file: the vectors and indexes are passed into another object\n",
    "def read_word_embeddings(embeddings_file: str):\n",
    "    \"\"\"\n",
    "    Loads the given embeddings (ASCII-formatted) into a WordEmbeddings object. Augments this with an UNK embedding\n",
    "    that is the 0 vector. Reads in all embeddings with no filtering -- you should only use this for relativized\n",
    "    word embedding files.\n",
    "    :param embeddings_file: path to the file containing embeddings\n",
    "    :return: WordEmbeddings object reflecting the words and their embeddings\n",
    "    \"\"\"\n",
    "    f = open(embeddings_file)\n",
    "    word_indexer = Indexer()\n",
    "    vectors = []\n",
    "    # Make position 0 a PAD token, which can be useful if you\n",
    "    word_indexer.add_and_get_index(\"PAD\")\n",
    "    # Make position 1 the UNK token\n",
    "    word_indexer.add_and_get_index(\"UNK\")\n",
    "    for line in f:\n",
    "        if line.strip() != \"\":\n",
    "            space_idx = line.find(' ')\n",
    "            word = line[:space_idx]\n",
    "            numbers = line[space_idx+1:]\n",
    "            float_numbers = [float(number_str) for number_str in numbers.split()]\n",
    "            vector = np.array(float_numbers)\n",
    "            word_indexer.add_and_get_index(word)\n",
    "            # Append the PAD and UNK vectors to start. Have to do this weirdly because we need to read the first line\n",
    "            # of the file to see what the embedding dim is\n",
    "            if len(vectors) == 0:\n",
    "                vectors.append(np.zeros(vector.shape[0]))\n",
    "                vectors.append(np.zeros(vector.shape[0]))\n",
    "            vectors.append(vector)\n",
    "    f.close()\n",
    "    print(\"Read in \" + repr(len(word_indexer)) + \" vectors of size \" + repr(vectors[0].shape[0]))\n",
    "    # Turn vectors into a 2-D numpy array\n",
    "    return WordEmbeddings(word_indexer, np.array(vectors))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# define auxiliary function: the indexer\n",
    "class Indexer(object):\n",
    "    \"\"\"\n",
    "    Bijection between objects and integers starting at 0. Useful for mapping\n",
    "    labels, features, etc. into coordinates of a vector space.\n",
    "\n",
    "    Attributes:\n",
    "        objs_to_ints\n",
    "        ints_to_objs\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in range(0, len(self))])\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "\n",
    "    def get_object(self, index):\n",
    "        \"\"\"\n",
    "        :param index: integer index to look up\n",
    "        :return: Returns the object corresponding to the particular index or None if not found\n",
    "        \"\"\"\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "\n",
    "    def contains(self, object):\n",
    "        \"\"\"\n",
    "        :param object: object to look up\n",
    "        :return: Returns True if it is in the Indexer, False otherwise\n",
    "        \"\"\"\n",
    "        return self.index_of(object) != -1\n",
    "\n",
    "    def index_of(self, object):\n",
    "        \"\"\"\n",
    "        :param object: object to look up\n",
    "        :return: Returns -1 if the object isn't present, index otherwise\n",
    "        \"\"\"\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "\n",
    "    def add_and_get_index(self, object, add=True):\n",
    "        \"\"\"\n",
    "        Adds the object to the index if it isn't present, always returns a nonnegative index\n",
    "        :param object: object to look up or add\n",
    "        :param add: True by default, False if we shouldn't add the object. If False, equivalent to index_of.\n",
    "        :return: The index of the object\n",
    "        \"\"\"\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]\n",
    "\n",
    "# define auxiliary function: the WordEmbeddings object: which can look up the index of a word and look up the embedding of a word\n",
    "class WordEmbeddings:\n",
    "    \"\"\"\n",
    "    Wraps an Indexer and a list of 1-D numpy arrays where each position in the list is the vector for the corresponding\n",
    "    word in the indexer. The 0 vector is returned if an unknown word is queried.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_indexer, vectors):\n",
    "        self.word_indexer = word_indexer\n",
    "        self.vectors = vectors\n",
    "\n",
    "    def get_initialized_embedding_layer(self):\n",
    "        return torch.nn.Embedding.from_pretrained(torch.FloatTensor(self.vectors))\n",
    "\n",
    "    def get_embedding_length(self):\n",
    "        return len(self.vectors[0])\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        Returns the embedding for a given word\n",
    "        :param word: The word to look up\n",
    "        :return: The UNK vector if the word is not in the Indexer or the vector otherwise\n",
    "        \"\"\"\n",
    "        word_idx = self.word_indexer.index_of(word)\n",
    "        if word_idx != -1:\n",
    "            return self.vectors[word_idx]\n",
    "        else:\n",
    "            return self.vectors[self.word_indexer.index_of(\"UNK\")]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# examples\n",
    "word_embeddings=read_word_embeddings('path') # read embedding\n",
    "word_embeddings.get_embedding('you')\n",
    "word_embeddings.word_indexer.index_of('you')\n",
    "embedding_layer=torch.nn.Embedding.from_pretrained(vectors) #load pre-trained embedding layer, the vectors must be torch.tensor type."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ]
}