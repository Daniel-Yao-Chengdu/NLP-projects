{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Load embeddings.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyMg8v6AqgOtYpvccoL+JCxh",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Daniel-Yao-Chengdu/NLP-project/blob/master/Load_static_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pretrained vectors here\n",
    "* https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html\n",
    "* we can download different vectors from the link above"
   ],
   "metadata": {
    "id": "VO2jFFYq3PG_",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fasttext embedding"
   ],
   "metadata": {
    "id": "95H97evip9fH",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmHHEvPNpl8K",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# download fasttext pretrained embeddings\n",
    "import os\n",
    "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
    "FILE = \"fastText\"\n",
    "\n",
    "if os.path.isdir(FILE):\n",
    "    print(\"fastText exists.\")\n",
    "else:\n",
    "    !wget -P $FILE $URL\n",
    "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create our own vocab\n",
    "!pip install NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentence = \"I love you, but also love NLP incredibly\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "vocab=set(tokens)\n",
    "vocab=list(vocab)"
   ],
   "metadata": {
    "id": "p-249GqTqzID",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Create a dictionary of embeddings. Here we only create the embedding dic that appear in our own vocab. \n",
    "#We can also create an embedding dict that involves all the original words.\n",
    "import torch\n",
    "fname=\"fastText/crawl-300d-2M.vec\"\n",
    "fin=open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "n, d = map(int, fin.readline().split())\n",
    "embedding_dic={}\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "for line in tqdm_notebook(fin):\n",
    "  tokens=line.rstrip().split(' ')\n",
    "  if tokens[0] in vocab: \n",
    "    embedding_dic[tokens[0]]=torch.tensor(list(map(float, tokens[1:]))).unsqueeze(0)"
   ],
   "metadata": {
    "id": "rR30L_IDqBEG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_dic"
   ],
   "metadata": {
    "id": "WBZHZ9VMtKCF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2vec"
   ],
   "metadata": {
    "id": "ls9DTjQBuWEC",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!we need to download the GoogleNews-vectors-negative300.bin file from the website above, and then upload it into colab. \n",
    "!gunzip GoogleNews-vectors-negative300.bin"
   ],
   "metadata": {
    "id": "kdtuyrdlw_jw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format ('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# if you vector file is in binary format, change to binary=True\n",
    "sentence = [\"London\", \"is\", \"the\", \"capital\", \"of\", \"Great\", \"Britain\"]\n",
    "vectors = [model[w] for w in sentence]"
   ],
   "metadata": {
    "id": "H4WV_M3xuXy2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Glove\n",
    "* This zip file contains 4 files for 4 embedding representations.\n",
    "* After unzipping the downloaded file we find four txt files: glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, and glove.6B.300d.txt. As their filenames suggests, they have vectors with different dimensions."
   ],
   "metadata": {
    "id": "Yjhh_vkO-n_w",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip"
   ],
   "metadata": {
    "id": "RusZ53IR-mjW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "embedding_dic = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_dic[word] = coefs\n",
    "f.close()"
   ],
   "metadata": {
    "id": "0D8e-AAV-tNo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_dic['love']"
   ],
   "metadata": {
    "id": "gMna45to_1Sr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load pretrained vectors to a torch nn.Embedding\n",
    "* refer to the NLP course HW1 for detailed explanation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# read vectors from Glove file: the vectors and indexes are passed into another object\n",
    "def read_word_embeddings(embeddings_file: str):\n",
    "    \"\"\"\n",
    "    Loads the given embeddings (ASCII-formatted) into a WordEmbeddings object. Augments this with an UNK embedding\n",
    "    that is the 0 vector. Reads in all embeddings with no filtering -- you should only use this for relativized\n",
    "    word embedding files.\n",
    "    :param embeddings_file: path to the file containing embeddings\n",
    "    :return: WordEmbeddings object reflecting the words and their embeddings\n",
    "    \"\"\"\n",
    "    f = open(embeddings_file)\n",
    "    word_indexer = Indexer()\n",
    "    vectors = []\n",
    "    # Make position 0 a PAD token, which can be useful if you\n",
    "    word_indexer.add_and_get_index(\"PAD\")\n",
    "    # Make position 1 the UNK token\n",
    "    word_indexer.add_and_get_index(\"UNK\")\n",
    "    for line in f:\n",
    "        if line.strip() != \"\":\n",
    "            space_idx = line.find(' ')\n",
    "            word = line[:space_idx]\n",
    "            numbers = line[space_idx+1:]\n",
    "            float_numbers = [float(number_str) for number_str in numbers.split()]\n",
    "            vector = np.array(float_numbers)\n",
    "            word_indexer.add_and_get_index(word)\n",
    "            # Append the PAD and UNK vectors to start. Have to do this weirdly because we need to read the first line\n",
    "            # of the file to see what the embedding dim is\n",
    "            if len(vectors) == 0:\n",
    "                vectors.append(np.zeros(vector.shape[0]))\n",
    "                vectors.append(np.zeros(vector.shape[0]))\n",
    "            vectors.append(vector)\n",
    "    f.close()\n",
    "    print(\"Read in \" + repr(len(word_indexer)) + \" vectors of size \" + repr(vectors[0].shape[0]))\n",
    "    # Turn vectors into a 2-D numpy array\n",
    "    return WordEmbeddings(word_indexer, np.array(vectors))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# define auxiliary function: the indexer\n",
    "class Indexer(object):\n",
    "    \"\"\"\n",
    "    Bijection between objects and integers starting at 0. Useful for mapping\n",
    "    labels, features, etc. into coordinates of a vector space.\n",
    "\n",
    "    Attributes:\n",
    "        objs_to_ints\n",
    "        ints_to_objs\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in range(0, len(self))])\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "\n",
    "    def get_object(self, index):\n",
    "        \"\"\"\n",
    "        :param index: integer index to look up\n",
    "        :return: Returns the object corresponding to the particular index or None if not found\n",
    "        \"\"\"\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "\n",
    "    def contains(self, object):\n",
    "        \"\"\"\n",
    "        :param object: object to look up\n",
    "        :return: Returns True if it is in the Indexer, False otherwise\n",
    "        \"\"\"\n",
    "        return self.index_of(object) != -1\n",
    "\n",
    "    def index_of(self, object):\n",
    "        \"\"\"\n",
    "        :param object: object to look up\n",
    "        :return: Returns -1 if the object isn't present, index otherwise\n",
    "        \"\"\"\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "\n",
    "    def add_and_get_index(self, object, add=True):\n",
    "        \"\"\"\n",
    "        Adds the object to the index if it isn't present, always returns a nonnegative index\n",
    "        :param object: object to look up or add\n",
    "        :param add: True by default, False if we shouldn't add the object. If False, equivalent to index_of.\n",
    "        :return: The index of the object\n",
    "        \"\"\"\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]\n",
    "\n",
    "# define auxiliary function: the WordEmbeddings object: which can look up the index of a word and look up the embedding of a word\n",
    "class WordEmbeddings:\n",
    "    \"\"\"\n",
    "    Wraps an Indexer and a list of 1-D numpy arrays where each position in the list is the vector for the corresponding\n",
    "    word in the indexer. The 0 vector is returned if an unknown word is queried.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_indexer, vectors):\n",
    "        self.word_indexer = word_indexer\n",
    "        self.vectors = vectors\n",
    "\n",
    "    def get_initialized_embedding_layer(self):\n",
    "        return torch.nn.Embedding.from_pretrained(torch.FloatTensor(self.vectors))\n",
    "\n",
    "    def get_embedding_length(self):\n",
    "        return len(self.vectors[0])\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        Returns the embedding for a given word\n",
    "        :param word: The word to look up\n",
    "        :return: The UNK vector if the word is not in the Indexer or the vector otherwise\n",
    "        \"\"\"\n",
    "        word_idx = self.word_indexer.index_of(word)\n",
    "        if word_idx != -1:\n",
    "            return self.vectors[word_idx]\n",
    "        else:\n",
    "            return self.vectors[self.word_indexer.index_of(\"UNK\")]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_word_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15592\\474557557.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# examples\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mword_embeddings\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mread_word_embeddings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'path'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# read embedding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mword_embeddings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_embedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'you'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mword_embeddings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mword_indexer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex_of\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'you'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0membedding_layer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvectors\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m#load pre-trained embedding layer, the vectors must be torch.tensor type.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'read_word_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "word_embeddings=read_word_embeddings('path') # read embedding\n",
    "word_embeddings.get_embedding('you')\n",
    "word_embeddings.word_indexer.index_of('you')\n",
    "embedding_layer=torch.nn.Embedding.from_pretrained(vectors) #load pre-trained embedding layer, the vectors must be torch.tensor type."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}