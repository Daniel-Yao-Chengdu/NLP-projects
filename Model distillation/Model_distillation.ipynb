{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model distillation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LYPSgxOVllyt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "YBwJA_YYJ1YJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0M_Hyhdsg0N"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The dataset statistics\n",
        "* 5269 samples\n",
        "* Number of labels: 1:115; 2:64; 3:109; 4:332; 5:4649. We should add weights to the crossentropy of the training set."
      ],
      "metadata": {
        "id": "L4nV15MSJWz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "import pandas as pd\n",
        "df=pd.read_json('All_Beauty_5.json', lines=True)\n",
        "texts = [' '.join([str(i),str(j)]) for i,j in zip(df['reviewText'],df['summary'])]\n",
        "original_labels = [i for i in df['overall']]\n",
        "\n",
        "#Next, we need to determine the number of labels in our data. We'll map each of these labels to an index.\n",
        "target_names = list(set(original_labels))\n",
        "label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
        "\n",
        "#We need to feed that data into the tokenizer, and the transform them into tensors\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "texts_tensor=[]\n",
        "for i in texts:\n",
        "  i=tokenizer(i,return_tensors=\"pt\", max_length=512, truncation=True)['input_ids']\n",
        "  texts_tensor.append(i)\n",
        "labels=[label2idx[i] for i in original_labels]"
      ],
      "metadata": {
        "id": "ipsKKutJukan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the dataset\n",
        "#divide into train, val, test\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_tensor, test_tensor, train_labels, test_labels = train_test_split(texts_tensor, labels , test_size=0.15, random_state=1)\n",
        "train_data=[[i,torch.tensor(j)] for i,j in zip(train_tensor,train_labels)]\n",
        "test_data=[[i,torch.tensor(j)] for i,j in zip(test_tensor,test_labels)]\n",
        "data=train_data+test_data"
      ],
      "metadata": {
        "id": "dW5GDdS-LX7S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train data statistics\n",
        "* number of samples: 4478\n",
        "* number of labels: 97, 54, 92, 285, 3950\n",
        "* proportion of labels: 2.1, 1.2, 2, 6, 88\n",
        "* weights for category: 46, 83, 49, 16, 1"
      ],
      "metadata": {
        "id": "mZyUnkjkNROP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define student model for distillation\n",
        "* Thoughts on designing student model:\n",
        "*1. To save time, I do not want to finetune the BERT model\n",
        "*2. I want the student model to mimic the behavior of the BERT model \n",
        "*3. There are several values to mimic: (1) logits of the final layer; (2) logits of several skip layers; (3) attention matrix; (4) value-value relation of the final layer; (5) cosine similarity of the softmax; (6) while distilling, consider add MLM task to the student model (trivial); (7) output softmax/logits of a specific task;\n",
        "*4. For simplicity, I distill (1) the logits of the final layer and (2) several layers, and (3) without distillation and then compare their behavior.\n",
        "\n"
      ],
      "metadata": {
        "id": "alMijXf5KDf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the teacher model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "teacher=AutoModel.from_pretrained('bert-base-uncased',output_hidden_states=True).to(device)\n",
        "\n",
        "#now, we want the layers to be 4\n",
        "config=AutoConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "config.num_hidden_layers=4\n",
        "\n",
        "#config the model with layer=4\n",
        "model = AutoModel.from_pretrained('bert-base-uncased', config=config).to(device)  # auto skip unused layers\n",
        "\n",
        "# now we want to initialize the parameters to be original layers [0, 4, 7, 10], \"skip idea\"\n",
        "layers=[0, 4, 7, 10]\n",
        "for i in range(4):\n",
        "    model.base_model.encoder.layer[i] = teacher.base_model.encoder.layer[layers[i]]"
      ],
      "metadata": {
        "id": "C08piOvguBMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student model training strategy 1 training process and hyperparameter initialization"
      ],
      "metadata": {
        "id": "LYPSgxOVllyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train our model, mimic the final layer logits\n",
        "# In this training, we want to the student model to even overfit the teacher model, so we do not set the model.train()\n",
        "def train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion, num_batch):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Training epoch {epoch+1}\")\n",
        "        loss_accumulate=0\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            inputs = batch[0].squeeze(0).to(device) #[m,512]\n",
        "            logits = model(inputs)[0] #[m,5] \n",
        "            targets= teacher(inputs)[0] #m\n",
        "            loss = criterion(logits,targets)/batchsize_grad\n",
        "            loss_accumulate+=loss.item()\n",
        "            loss.backward() #The gradients are computed when we call loss. backward() and are stored by PyTorch until we call optimizer.\n",
        "            \n",
        "            if (i+1) % batchsize_grad == 0: #when accumulated batch=16, we do optimizer after 16 batches of gradients are accumulated\n",
        "                optimizer.step()\n",
        "                #scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "                print (loss_accumulate)\n",
        "                loss_accumulate=0"
      ],
      "metadata": {
        "id": "iQE4m3HTykFh"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm, trange\n",
        "batch_size=1\n",
        "batchsize_grad=8\n",
        "epochs=1\n",
        "lr=5e-5\n",
        "num_batch=int(len(data)/batch_size)+1\n",
        "torch.manual_seed(0)\n",
        "train_dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "criterion=torch.nn.MSELoss(reduction='mean')\n",
        "torch.manual_seed(0)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*epochs)\n",
        "scheduler=None"
      ],
      "metadata": {
        "id": "g1OXrJd3yqos"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train 316 steps\n",
        "train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion, num_batch)"
      ],
      "metadata": {
        "id": "2pX2XOHgyvi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student model training strategy 2 training process and hyperparameter initialization"
      ],
      "metadata": {
        "id": "cQmuk2_mxAGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We force the outputs of every layer to mimic the teacher model. "
      ],
      "metadata": {
        "id": "2EQDuRpPydst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train our model, mimic the final layer logits\n",
        "# In this training, we want to the student model to even overfit the teacher model, so we do not set the model.train()\n",
        "def train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion, num_batch):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Training epoch {epoch+1}\")\n",
        "        loss_accumulate=0\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            inputs = batch[0].squeeze(0).to(device) #[m,512]\n",
        "            logits = model(inputs) #[m,5] \n",
        "            targets = teacher(inputs) #m\n",
        "            loss1 = criterion(logits[0],targets[0]) #output layer\n",
        "            loss2 = criterion(logits[2][1],targets[2][1]) #1st layer\n",
        "            loss3 = criterion(logits[2][2],targets[2][5]) #2nd layer\n",
        "            loss4 = criterion(logits[2][3],targets[2][8]) #3nd layer\n",
        "            loss=(loss1+loss2+loss3+loss4)/4/batchsize_grad\n",
        "            loss_accumulate+=loss.item()\n",
        "            loss.backward() #The gradients are computed when we call loss. backward() and are stored by PyTorch until we call optimizer.\n",
        "            if (i+1) % batchsize_grad == 0: #when accumulated batch=16, we do optimizer after 16 batches of gradients are accumulated\n",
        "                optimizer.step()\n",
        "                #scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "                print (i+1, loss_accumulate)\n",
        "                loss_accumulate=0\n",
        "            if i+2 == 2529: return 'finish training' "
      ],
      "metadata": {
        "id": "DD6CFVI8xbCG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm, trange\n",
        "batch_size=1\n",
        "batchsize_grad=8\n",
        "epochs=1\n",
        "lr=5e-5\n",
        "num_batch=int(len(data)/batch_size)+1\n",
        "torch.manual_seed(0)\n",
        "train_dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "criterion=torch.nn.MSELoss(reduction='mean')\n",
        "torch.manual_seed(0)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*epochs)\n",
        "scheduler=None"
      ],
      "metadata": {
        "id": "vTesf6g80sSc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train 316 steps\n",
        "train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion, num_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m1VL5DkN0yPK",
        "outputId": "7ce451fd-da8c-440e-eee0-58e30579c329"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1\n",
            "8 0.27703865990042686\n",
            "16 0.2574931662529707\n",
            "24 0.23059661500155926\n",
            "32 0.1960786897689104\n",
            "40 0.1836695373058319\n",
            "48 0.18020579777657986\n",
            "56 0.17036115378141403\n",
            "64 0.1645518783479929\n",
            "72 0.15668603591620922\n",
            "80 0.14586523547768593\n",
            "88 0.140483851544559\n",
            "96 0.1345058148726821\n",
            "104 0.14169701095670462\n",
            "112 0.13230726029723883\n",
            "120 0.10801821760833263\n",
            "128 0.11074337922036648\n",
            "136 0.10426778253167868\n",
            "144 0.08749044500291348\n",
            "152 0.10336858965456486\n",
            "160 0.08897718321532011\n",
            "168 0.08065301645547152\n",
            "176 0.08372785802930593\n",
            "184 0.07825937308371067\n",
            "192 0.07347073871642351\n",
            "200 0.06771460920572281\n",
            "208 0.06302279140800238\n",
            "216 0.06019698875024915\n",
            "224 0.056106350384652615\n",
            "232 0.05015338538214564\n",
            "240 0.04993952717632055\n",
            "248 0.04918390139937401\n",
            "256 0.04726535640656948\n",
            "264 0.048852693289518356\n",
            "272 0.04090201808139682\n",
            "280 0.040910341776907444\n",
            "288 0.04241461306810379\n",
            "296 0.035019478760659695\n",
            "304 0.03433313174173236\n",
            "312 0.03345872391946614\n",
            "320 0.029013865860179067\n",
            "328 0.02918864064849913\n",
            "336 0.02684640185907483\n",
            "344 0.024863464757800102\n",
            "352 0.02691080840304494\n",
            "360 0.02740651718340814\n",
            "368 0.02427377551794052\n",
            "376 0.02311362256295979\n",
            "384 0.02152850409038365\n",
            "392 0.020799728808924556\n",
            "400 0.01987546239979565\n",
            "408 0.019785062409937382\n",
            "416 0.01844279794022441\n",
            "424 0.017942042089998722\n",
            "432 0.017142138327471912\n",
            "440 0.01598916156217456\n",
            "448 0.016939123976044357\n",
            "456 0.01489497534930706\n",
            "464 0.014729659771546721\n",
            "472 0.015277044149115682\n",
            "480 0.01547515532001853\n",
            "488 0.01424196525476873\n",
            "496 0.013331398251466453\n",
            "504 0.012808593106456101\n",
            "512 0.012226532446220517\n",
            "520 0.013680487521924078\n",
            "528 0.010941889486275613\n",
            "536 0.011341544101014733\n",
            "544 0.011153514147736132\n",
            "552 0.010284693678840995\n",
            "560 0.011812854092568159\n",
            "568 0.010924619040451944\n",
            "576 0.010520148789510131\n",
            "584 0.010233891545794904\n",
            "592 0.009735156316310167\n",
            "600 0.009938540169969201\n",
            "608 0.00909448298625648\n",
            "616 0.008985898341052234\n",
            "624 0.009531555813737214\n",
            "632 0.008886799740139395\n",
            "640 0.009447913500480354\n",
            "648 0.008115920471027493\n",
            "656 0.008122998697217554\n",
            "664 0.007970069244038314\n",
            "672 0.008188950188923627\n",
            "680 0.008299325010739267\n",
            "688 0.007571780646685511\n",
            "696 0.0074426872306503356\n",
            "704 0.006644207693170756\n",
            "712 0.006853358470834792\n",
            "720 0.006609403237234801\n",
            "728 0.006477780232671648\n",
            "736 0.006482992961537093\n",
            "744 0.00649993511615321\n",
            "752 0.005839041201397777\n",
            "760 0.00654949841555208\n",
            "768 0.005976752087008208\n",
            "776 0.00592528295237571\n",
            "784 0.005839814897626638\n",
            "792 0.005828392051625997\n",
            "800 0.005543121544178575\n",
            "808 0.006061008956748992\n",
            "816 0.005407338903751224\n",
            "824 0.005016642215196043\n",
            "832 0.004878834442934021\n",
            "840 0.004652686649933457\n",
            "848 0.004442777979420498\n",
            "856 0.004934670345392078\n",
            "864 0.00446376932086423\n",
            "872 0.003967071155784652\n",
            "880 0.004258258763002232\n",
            "888 0.003974294639192522\n",
            "896 0.004032987839309499\n",
            "904 0.003593838308006525\n",
            "912 0.0034588671114761382\n",
            "920 0.00392680853838101\n",
            "928 0.0032557363156229258\n",
            "936 0.0033386506838724017\n",
            "944 0.003909127757651731\n",
            "952 0.0029780549521092325\n",
            "960 0.0029779188917018473\n",
            "968 0.0033958613639697433\n",
            "976 0.0030483082809951156\n",
            "984 0.0032588104368187487\n",
            "992 0.0033346353156957775\n",
            "1000 0.002759507711743936\n",
            "1008 0.003338231414090842\n",
            "1016 0.0026725527714006603\n",
            "1024 0.003134778671665117\n",
            "1032 0.00274440337670967\n",
            "1040 0.002703019796172157\n",
            "1048 0.0027654767618514597\n",
            "1056 0.0026587100292090327\n",
            "1064 0.0024451599310850725\n",
            "1072 0.002317627251613885\n",
            "1080 0.0022537824843311682\n",
            "1088 0.00238812412135303\n",
            "1096 0.002203804499004036\n",
            "1104 0.002315336692845449\n",
            "1112 0.001711968594463542\n",
            "1120 0.001984028029255569\n",
            "1128 0.002050502414931543\n",
            "1136 0.0021548081131186336\n",
            "1144 0.0017489567107986659\n",
            "1152 0.0022432791447499767\n",
            "1160 0.0017711427062749863\n",
            "1168 0.002026603542617522\n",
            "1176 0.0018471099174348637\n",
            "1184 0.001766835237503983\n",
            "1192 0.0018534815171733499\n",
            "1200 0.001656049003941007\n",
            "1208 0.0016607065626885742\n",
            "1216 0.001709683332592249\n",
            "1224 0.0017767944373190403\n",
            "1232 0.0018942047900054604\n",
            "1240 0.0017999249539570883\n",
            "1248 0.0016873860877240077\n",
            "1256 0.0015897208650130779\n",
            "1264 0.0014118230028543621\n",
            "1272 0.0014812279550824314\n",
            "1280 0.0014958902029320598\n",
            "1288 0.0014351222198456526\n",
            "1296 0.0012785775470547378\n",
            "1304 0.0013057988253422081\n",
            "1312 0.0014480330937658437\n",
            "1320 0.0012915679981233552\n",
            "1328 0.0011747586249839514\n",
            "1336 0.0012817866954719648\n",
            "1344 0.001329611535766162\n",
            "1352 0.0013252647113404237\n",
            "1360 0.001417677958670538\n",
            "1368 0.0014038137742318213\n",
            "1376 0.0011798928899224848\n",
            "1384 0.0013176640422898345\n",
            "1392 0.001406803450663574\n",
            "1400 0.0016669195319991559\n",
            "1408 0.0013964766112621874\n",
            "1416 0.001206005057611037\n",
            "1424 0.0012732832110486925\n",
            "1432 0.0011956127782468684\n",
            "1440 0.0012567068260977976\n",
            "1448 0.0011184146715095267\n",
            "1456 0.001111060795665253\n",
            "1464 0.001313137930992525\n",
            "1472 0.0010284258678439073\n",
            "1480 0.0009748902448336594\n",
            "1488 0.0010708842164603993\n",
            "1496 0.0009733629194670357\n",
            "1504 0.0009721009264467284\n",
            "1512 0.0010296956825186498\n",
            "1520 0.0008802223092061467\n",
            "1528 0.0011704517746693455\n",
            "1536 0.0010237505703116767\n",
            "1544 0.0012077960491296835\n",
            "1552 0.001163488363090437\n",
            "1560 0.0009114643253269605\n",
            "1568 0.000967665713687893\n",
            "1576 0.0010343461835873313\n",
            "1584 0.0010299449568265118\n",
            "1592 0.0009994312931667082\n",
            "1600 0.001304368423006963\n",
            "1608 0.0008817719499347731\n",
            "1616 0.001013042507111095\n",
            "1624 0.0010165806670556776\n",
            "1632 0.0010623875859891996\n",
            "1640 0.000761723676987458\n",
            "1648 0.0009259826911147684\n",
            "1656 0.0008810328727122396\n",
            "1664 0.0008605234252172522\n",
            "1672 0.0008358461564057507\n",
            "1680 0.0008209452062146738\n",
            "1688 0.0010830082683241926\n",
            "1696 0.0009612971989554353\n",
            "1704 0.0007236215751618147\n",
            "1712 0.0007475542661268264\n",
            "1720 0.0009004336752695963\n",
            "1728 0.0008814075044938363\n",
            "1736 0.0009097000365727581\n",
            "1744 0.000869926021550782\n",
            "1752 0.0008325043309014291\n",
            "1760 0.0008561171998735517\n",
            "1768 0.0008397576457355171\n",
            "1776 0.00081020387733588\n",
            "1784 0.000836913641251158\n",
            "1792 0.0006742229234077968\n",
            "1800 0.0007250704074976966\n",
            "1808 0.0007266928296303377\n",
            "1816 0.000774672647821717\n",
            "1824 0.000733474844309967\n",
            "1832 0.001074070380127523\n",
            "1840 0.0006759907410014421\n",
            "1848 0.0006772678170818835\n",
            "1856 0.0008800022187642753\n",
            "1864 0.0008351893920917064\n",
            "1872 0.0006425756073440425\n",
            "1880 0.0007227259047795087\n",
            "1888 0.0005781671316071879\n",
            "1896 0.0006823556213930715\n",
            "1904 0.0007069846906233579\n",
            "1912 0.0006172971698106267\n",
            "1920 0.0006758421659469604\n",
            "1928 0.0007942458032630384\n",
            "1936 0.0006478266441263258\n",
            "1944 0.000655240481137298\n",
            "1952 0.0006548902383656241\n",
            "1960 0.0009433990198886022\n",
            "1968 0.0007171038087108172\n",
            "1976 0.0006519652852148283\n",
            "1984 0.0007146570678742137\n",
            "1992 0.0007735230028629303\n",
            "2000 0.0006882131565362215\n",
            "2008 0.0005441048451757524\n",
            "2016 0.000615935554378666\n",
            "2024 0.0006171631794131827\n",
            "2032 0.0006233411477296613\n",
            "2040 0.0009322490477643441\n",
            "2048 0.000530434976099059\n",
            "2056 0.0006003803064231761\n",
            "2064 0.0006646524270763621\n",
            "2072 0.0006799653674534056\n",
            "2080 0.000608174716035137\n",
            "2088 0.00052441772641032\n",
            "2096 0.0006918079634488095\n",
            "2104 0.0005618783579848241\n",
            "2112 0.0005728932046622504\n",
            "2120 0.0005693575258192141\n",
            "2128 0.0005671043290931266\n",
            "2136 0.0005741142376791686\n",
            "2144 0.0005950540326011833\n",
            "2152 0.0005438638363557402\n",
            "2160 0.0006108501729613636\n",
            "2168 0.00047426740275113843\n",
            "2176 0.0005857091200596187\n",
            "2184 0.0005645153178193141\n",
            "2192 0.0005490203511726577\n",
            "2200 0.0005486537702381611\n",
            "2208 0.0005838404613314196\n",
            "2216 0.0006173818073875736\n",
            "2224 0.000536105282662902\n",
            "2232 0.0006328015551844146\n",
            "2240 0.000577763650653651\n",
            "2248 0.0006769145329599269\n",
            "2256 0.0005004261511203367\n",
            "2264 0.0006174116024340037\n",
            "2272 0.0007026361636235379\n",
            "2280 0.00045867168228141963\n",
            "2288 0.0005222065046837088\n",
            "2296 0.0005723766080336645\n",
            "2304 0.0006464632242568769\n",
            "2312 0.0004669665177061688\n",
            "2320 0.0005067998936283402\n",
            "2328 0.0004261323410901241\n",
            "2336 0.0005182179738767445\n",
            "2344 0.0006234984030015767\n",
            "2352 0.0006049265648471192\n",
            "2360 0.0005374041975301225\n",
            "2368 0.0006790865190851036\n",
            "2376 0.0005771398064098321\n",
            "2384 0.0004857349340454675\n",
            "2392 0.0004956168922944926\n",
            "2400 0.00045389134538709186\n",
            "2408 0.00045584388863062486\n",
            "2416 0.0004492074658628553\n",
            "2424 0.0005780194223916624\n",
            "2432 0.00036379893936100416\n",
            "2440 0.0004341214989835862\n",
            "2448 0.0004570197306748014\n",
            "2456 0.000410615870350739\n",
            "2464 0.00045344273530645296\n",
            "2472 0.00039211793409776874\n",
            "2480 0.0004134086047997698\n",
            "2488 0.00037984726805007085\n",
            "2496 0.0004533096434897743\n",
            "2504 0.00046745515282964334\n",
            "2512 0.0004668711371778045\n",
            "2520 0.0004260726418578997\n",
            "2528 0.00037001277451054193\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'finish training'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student model classification finetune"
      ],
      "metadata": {
        "id": "x2O2cWB4wlgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define classification model"
      ],
      "metadata": {
        "id": "0GoD72JE9aEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a student+Linear model\n",
        "class BaseLinear(torch.nn.Module):\n",
        "    def __init__(self, basemodel): \n",
        "        super().__init__()\n",
        "        self.basemodel=basemodel\n",
        "        self.linear = torch.nn.Linear(in_features=768, out_features=5)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x=self.basemodel(x)[1] \n",
        "        #for name, param in self.basemodel.state_dict().items():\n",
        "        #  if name!=\"0\": param.requires_grad=False\n",
        "        logits = self.linear(x) # [m,5]\n",
        "        return logits #(m,5)\n",
        "basemodel=model.to(device)\n",
        "torch.manual_seed(0)\n",
        "final_model=BaseLinear(basemodel=basemodel).to(device)"
      ],
      "metadata": {
        "id": "6t4vcPTEzqwu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define training process and hyperparameter init"
      ],
      "metadata": {
        "id": "kOP3YFT3mA1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train our model\n",
        "def train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion, val_dataloader,len_val):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Training epoch {epoch+1}\")\n",
        "        model.train()\n",
        "        loss_accumulate=0\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            model.train()\n",
        "            inputs=batch[0].squeeze(0).to(device) #[m,512]\n",
        "            logits = model(inputs).view(1,-1) #[m,5]\n",
        "            targets=batch[1].to(device) #m\n",
        "            loss = criterion(logits,targets)/batchsize_grad\n",
        "            loss_accumulate+=loss\n",
        "            loss.backward() #The gradients are computed when we call loss. backward() and are stored by PyTorch until we call optimizer.\n",
        "            \n",
        "            if (i+1) % batchsize_grad == 0: #when accumulated batch=16, we do optimizer after 16 batches of gradients are accumulated\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "                print (i+1, loss_accumulate)\n",
        "                loss_accumulate=0\n",
        "                \n",
        "\n",
        "        #for evaluate the model after an epoch\n",
        "        model.eval()\n",
        "        accuracy=0\n",
        "        for i, batch in enumerate(val_dataloader):\n",
        "            inputs=batch[0].squeeze(0).to(device) #[m,512]\n",
        "            with torch.no_grad():\n",
        "              logits = model(inputs) #[m,5]\n",
        "            softmaxed=torch.softmax(logits,-1) #[m,5]\n",
        "            predict_label=torch.argmax(softmaxed,-1).to('cpu')\n",
        "            targets=batch[1].to('cpu') #m\n",
        "            from sklearn.metrics import accuracy_score\n",
        "            accuracy+=accuracy_score(targets,predict_label)*batch[0].shape[0]\n",
        "        print (\"accuracy\",accuracy/len_val)"
      ],
      "metadata": {
        "id": "emYYarlR9nFS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path=\"state_dict_model_1.pt\"\n",
        "#model.load_state_dict(torch.load(path))\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "batch_size=1\n",
        "batchsize_grad=5\n",
        "epochs=1\n",
        "lr=5e-5\n",
        "len_val=len(test_data)\n",
        "torch.manual_seed(0)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
        "criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor([46, 83, 49, 16, 1],dtype=torch.float))\n",
        "torch.manual_seed(0)\n",
        "optimizer = AdamW(final_model.parameters(), lr=lr)\n",
        "torch.manual_seed(0)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*epochs/batchsize_grad)"
      ],
      "metadata": {
        "id": "OUQcgT8r90lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataloader,final_model,batchsize_grad,epochs,scheduler,optimizer,criterion,val_dataloader, len_val)"
      ],
      "metadata": {
        "id": "EDwiByN3-J3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For training strategy 1, after training 1 epoch, the test accuracy is 92.67, the train accuracy is 94.5. There might be improvement space if it is trained with more epochs.\n",
        "* For training strategy 2, after training 1 epoch, the test accuracy is 92.79, the train accuracy is 93.39\n",
        "* For no distillation, after training 1 epoch, the test accuracy is 94.31, the train accuracy is 97. "
      ],
      "metadata": {
        "id": "A0oUVb02wDco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for evaluate the final_model after an epoch\n",
        "model.eval()\n",
        "accuracy=0\n",
        "val_dataloader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
        "\n",
        "for i, batch in enumerate(val_dataloader):\n",
        "    inputs=batch[0].squeeze(0).to(device) #[m,512]\n",
        "    with torch.no_grad():\n",
        "      logits = final_model(inputs) #[m,5]\n",
        "    softmaxed=torch.softmax(logits,-1) #[m,5]\n",
        "    predict_label=torch.argmax(softmaxed,-1).to('cpu')\n",
        "    targets=batch[1].to('cpu') #m\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    accuracy+=accuracy_score(targets,predict_label)*batch[0].shape[0]\n",
        "print (\"accuracy\",accuracy/len(val_dataloader))"
      ],
      "metadata": {
        "id": "6cpVlU8jAJya"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}