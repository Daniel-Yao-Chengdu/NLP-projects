{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.0 Install and import library"
      ],
      "metadata": {
        "id": "ag5kres6XLaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install fire"
      ],
      "metadata": {
        "id": "wrDXqD7VBhNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM_M8m1jBOYo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.0 Prepare dataset into the train, val, test: [text_tensor, label_tensor]"
      ],
      "metadata": {
        "id": "1_6ueCTUWB-U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq0ZM7y6BOYr"
      },
      "source": [
        "## Read original texts, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWtkrJZ4BOYv"
      },
      "outputs": [],
      "source": [
        "# Load original texts and labels\n",
        "df=pd.read_json('All_Beauty_5.json', lines=True)\n",
        "texts = [' '.join([str(i),str(j)]) for i,j in zip(df['reviewText'],df['summary'])]\n",
        "labels = [i for i in df['overall']]\n",
        "\n",
        "# Classes are imbalanced, so we need to remove some samples. \n",
        "new_texts=[]\n",
        "new_labels=[]\n",
        "N_5=0\n",
        "for i in range(len(labels)):\n",
        "  if labels[i]==5:\n",
        "    N_5+=1\n",
        "    if N_5<156:new_texts.append(texts[i]);new_labels.append(labels[i])\n",
        "  else: new_texts.append(texts[i]);new_labels.append(labels[i])\n",
        "\n",
        "# Assign new texts and labels as our dataset\n",
        "texts=new_texts\n",
        "labels=new_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOjl_KpRBOYw"
      },
      "source": [
        "## Create a list: `[data_tensor, label_tensor]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Go81HZpBOYw",
        "outputId": "31e8a0aa-74c0-4954-af1e-b6b299f24cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n"
          ]
        }
      ],
      "source": [
        "#Next, we need to determine the number of labels in our data. We'll map each of these labels to an index.\n",
        "target_names = list(set(labels))\n",
        "label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
        "print(label2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZNLwqHRBOYx"
      },
      "outputs": [],
      "source": [
        "#Create text tensors. \n",
        "#Because we only want to train the linear layers, so we can firstly feed all data into Bert tokenizer, and the transform all the data into tensors.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "texts_tensor=tokenizer(texts,padding=True, max_length=512, truncation=True, return_tensors=\"pt\")['input_ids']\n",
        "\n",
        "#Set BERT model, note that: model.eval() and with_no_grad\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bertmodel=BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "dataloader = DataLoader(texts_tensor, batch_size=1, shuffle=False)\n",
        "bertmodel.eval()\n",
        "\n",
        "#Feed the model with our data\n",
        "logits_bert=torch.zeros(775,768)\n",
        "for i, batch in enumerate(dataloader):\n",
        "    inputs=batch[0].to(device) #[m,512]\n",
        "    with torch.no_grad():\n",
        "      logits = bertmodel(inputs.unsqueeze(0))[1] #[m,768]\n",
        "    logits_bert[i]=logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create data list, which can be used for dataloader. \n",
        "labels_tensor=[torch.tensor(label2idx[i]) for i in labels]\n",
        "data=[[i, j]for i,j in zip (logits_bert,labels_tensor)]\n",
        "#path=\"data.pt\"\n",
        "#data=torch.load(path)"
      ],
      "metadata": {
        "id": "6zUg6hotRy_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divide into train, val, test"
      ],
      "metadata": {
        "id": "2JMMKcHSx9Vt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1IHveOkBOYz"
      },
      "outputs": [],
      "source": [
        "#divide into train, val, test\n",
        "from sklearn.model_selection import train_test_split\n",
        "rest_data, test_data = train_test_split(data, test_size=0.1, random_state=0)\n",
        "train_data, val_data = train_test_split(rest_data, test_size=0.1, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpA2EWnsBOYz"
      },
      "source": [
        "# 3.0 Create the model and divide the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create our model using Class"
      ],
      "metadata": {
        "id": "60PNZAimWzNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWXcNn_NBOY0"
      },
      "outputs": [],
      "source": [
        "#create a BERT + Linear model\n",
        "class BertLinear(torch.nn.Module):\n",
        "    def __init__(self): \n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(in_features=768, out_features=5, bias=True)\n",
        "        #self.linear2 = torch.nn.Linear(in_features=1024, out_features=512, bias=True)\n",
        "        #self.linear3 = torch.nn.Linear(in_features=200, out_features=5, bias=True)\n",
        "\n",
        "    def forward(self, x): #input_ids,[m,768] \n",
        "        x=self.linear1(x)\n",
        "        #x=torch.nn.functional.relu(self.linear2(x))\n",
        "        #x=self.linear3(x)\n",
        "        return x #(m,5)\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model=BertLinear().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the training process"
      ],
      "metadata": {
        "id": "fRr7njtpyIMe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxqSVq1zBOY2"
      },
      "outputs": [],
      "source": [
        "#train our model\n",
        "def train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion, num_batch, val_dataloader,len_val):\n",
        "\n",
        "    acc_steps = 100\n",
        "    model.train()\n",
        "\n",
        "    accumulating_batch_count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            model.train()\n",
        "            inputs=batch[0].to(device) #[m,768]\n",
        "            logits = model(inputs) #[m,5] \n",
        "            targets=batch[1].to(device) #m\n",
        "            loss = criterion(logits,targets)\n",
        "            loss.backward() #The gradients are computed when we call loss. backward() and are stored by PyTorch until we call optimizer.\n",
        "\n",
        "            if accumulating_batch_count % batchsize_grad == 0: #when accumulated batch=16, we do optimizer after 16 batches of gradients are accumulated\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "                #if i%50==0: print (i-num_batch, loss.item())\n",
        "            accumulating_batch_count += 1\n",
        "\n",
        "            # for evaluate the model after certain batches\n",
        "            if accumulating_batch_count % len(train_dataloader)==0:\n",
        "                model.eval()\n",
        "                accuracy=0\n",
        "                for i, batch in enumerate(val_dataloader):\n",
        "                    inputs=batch[0].to(device) #[m,512]\n",
        "                    with torch.no_grad():\n",
        "                      logits = model(inputs) #[m,5]\n",
        "                    softmaxed=torch.softmax(logits,-1) #[m,5]\n",
        "                    predict_label=torch.argmax(softmaxed,-1).to('cpu')\n",
        "                    targets=batch[1].to('cpu') #m\n",
        "                    from sklearn.metrics import accuracy_score\n",
        "                    accuracy+=accuracy_score(targets,predict_label)*batch[0].shape[0]\n",
        "\n",
        "        #print the loss and accuracy of the validation set after each epoch\n",
        "        print (loss.item(),accuracy/len_val)\n",
        "\n",
        "        #save the best model\n",
        "        if accuracy/len_val>0.82: path=\"best_model.pt\"; torch.save(model.state_dict(), path) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.0 Start training:remember to save the model"
      ],
      "metadata": {
        "id": "RjTufCUkyKtv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7cq42T2BOY3"
      },
      "outputs": [],
      "source": [
        "#path=\"state_dict_model_1.pt\"\n",
        "#model.load_state_dict(torch.load(path))\n",
        "\n",
        "batch_size=627\n",
        "batchsize_grad=1\n",
        "epochs=4000 #simple model uses more epochs\n",
        "lr=0.008 #simple models uses larger lr\n",
        "len_val=len(val_data)\n",
        "num_batch=round(len(train_data)/batch_size)-1\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "criterion=torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=5000, num_training_steps=-1)\n",
        "train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion, num_batch,val_dataloader, len_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save our model parameters\n",
        "N+=1\n",
        "path=\"state_dict_model_\" + str(N) + \".pt\"\n",
        "torch.save(model.state_dict(), path) "
      ],
      "metadata": {
        "id": "5f8oSMAFiaK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.0 Validate our model"
      ],
      "metadata": {
        "id": "eScSw3UtfUpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path=\"state_dict_model_3.pt\" \n",
        "#model.load_state_dict(torch.load(path))\n",
        "#model.eval()\n",
        "\n",
        "input_data=val_data\n",
        "val_dataloader = DataLoader(input_data, batch_size=1000, shuffle=False)\n",
        "accuracy=0\n",
        "\n",
        "for i, batch in enumerate(val_dataloader):\n",
        "    inputs=batch[0].to(device) #[m,512]\n",
        "    with torch.no_grad():\n",
        "      logits = model(inputs) #[m,5]\n",
        "    softmaxed=torch.softmax(logits,-1) #[m,5]\n",
        "    predict_label=torch.argmax(softmaxed,-1).to('cpu')\n",
        "    targets=batch[1].to('cpu') #m\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    accuracy+=accuracy_score(targets,predict_label)*batch[0].shape[0]\n",
        "print (accuracy/len(input_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V1YNZuQptAY",
        "outputId": "e6781bac-72ff-41e3-e83d-d201022be614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9952153110047847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.0 Baseline model"
      ],
      "metadata": {
        "id": "WETQoVA8B9if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('lr', LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\"))\n",
        "])\n",
        "\n",
        "parameters = {'lr__C': [0.1, 0.5, 1, 2, 5, 10, 100, 1000]}\n",
        "\n",
        "i=500\n",
        "best_classifier = GridSearchCV(pipeline, parameters, cv=5, verbose=1)\n",
        "best_classifier.fit(texts[0:i], labels[0:i])\n",
        "best_predictions = best_classifier.predict(texts[i:])\n",
        "\n",
        "baseline_accuracy = np.mean(best_predictions == labels[i:])\n",
        "print(\"Baseline accuracy:\", baseline_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCPMZGu7B-7N",
        "outputId": "7a6c3434-0145-41ee-e791-33aea4465f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Baseline accuracy: 0.6981818181818182\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "(only linear )Bert for classification project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}