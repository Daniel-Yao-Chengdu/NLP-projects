{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextCnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "* Example: [here](https://chriskhanhtran.github.io/posts/cnn-sentence-classification/)"
      ],
      "metadata": {
        "id": "p8RO5XORarYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "4HCnPxpIwD7a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M-HDdCfFwAW0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F # for use some functions like F.relu(), F.dropout()\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preparation"
      ],
      "metadata": {
        "id": "yhqdlR-dwZAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General processing"
      ],
      "metadata": {
        "id": "1za5lgtZ0QMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For general processing of the data\n",
        "df=pd.read_json('drive/MyDrive/Colab Notebooks/All_Beauty_5.json', lines=True)\n",
        "texts = [' '.join([str(i),str(j)]) for i,j in zip(df['reviewText'],df['summary'])]\n",
        "labels = [i for i in df['overall']]\n",
        "\n",
        "new_texts=[]\n",
        "new_labels=[]\n",
        "N_5=0\n",
        "for i in range(len(labels)):\n",
        "  if labels[i]==5:\n",
        "    N_5+=1\n",
        "    if N_5<156:new_texts.append(texts[i]);new_labels.append(labels[i])\n",
        "  else: new_texts.append(texts[i]);new_labels.append(labels[i])\n",
        "texts=new_texts\n",
        "labels=new_labels\n",
        "\n",
        "#map labels to [0,1,2,3,4]\n",
        "target_names = list(set(labels))\n",
        "label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
        "labels=[(label2idx[i]) for i in labels]"
      ],
      "metadata": {
        "id": "z8eeXLWywaak"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization for text\n",
        "* Use NLTK for tokenization"
      ],
      "metadata": {
        "id": "XgUvBtb_0Xje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "zZUxnfeKx8JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lowercase, remove punctuation, tokenization \n",
        "new_texts=[]\n",
        "for i in texts:\n",
        "  new_texts.append(' '.join(w for w in word_tokenize(i.lower()) if w not in string.punctuation))\n",
        "texts=new_texts"
      ],
      "metadata": {
        "id": "58RiyOpo1UHH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings for text"
      ],
      "metadata": {
        "id": "-mTjTqCC_Q6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our own vocab because this would make computation faster\n",
        "text_combine=' '.join(texts)\n",
        "vocab=set(text_combine.split(' '))\n",
        "vocab=list(vocab)"
      ],
      "metadata": {
        "id": "eVhS_LXmrb9m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download fasttext/word2vec pretrained embeddings\n",
        "import os\n",
        "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
        "FILE = \"fastText\"\n",
        "\n",
        "if os.path.isdir(FILE):\n",
        "    print(\"fastText exists.\")\n",
        "else:\n",
        "    !wget -P $FILE $URL\n",
        "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
      ],
      "metadata": {
        "id": "Dp-jE5NRyfmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of embeddings of our corpus\n",
        "fname=\"fastText/crawl-300d-2M.vec\"\n",
        "fin=open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "n, d = map(int, fin.readline().split())\n",
        "embedding_dic={}\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "for line in tqdm_notebook(fin):\n",
        "  tokens=line.rstrip().split(' ')\n",
        "  if tokens[0] in vocab: embedding_dic[tokens[0]]=torch.tensor(list(map(float, tokens[1:]))).unsqueeze(0)"
      ],
      "metadata": {
        "id": "ouHsYdi_sKr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the embeddings for our text\n",
        "text_embedding=[]\n",
        "for sentence in texts:\n",
        "  sentence_embedding=torch.zeros(1,300)\n",
        "  for word in sentence.split(' '):\n",
        "    if word in [*embedding_dic]: \n",
        "      sentence_embedding=torch.cat((sentence_embedding,embedding_dic[word]),0)\n",
        "  text_embedding.append(sentence_embedding[1:])"
      ],
      "metadata": {
        "id": "ewzK7WXPrPbl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the final form and divide"
      ],
      "metadata": {
        "id": "ET0NU7bl7JRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=[[i, torch.tensor(j)]for i,j in zip (text_embedding,labels)]"
      ],
      "metadata": {
        "id": "VUbuCTdV67r1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# divide\n",
        "from sklearn.model_selection import train_test_split\n",
        "rest_data, test_data = train_test_split(data, test_size=0.1, random_state=1)\n",
        "train_data, val_data = train_test_split(rest_data, test_size=0.1, random_state=1)"
      ],
      "metadata": {
        "id": "2-1haeHS6-7f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define our model and train"
      ],
      "metadata": {
        "id": "sTMu6fFx8FGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_NLP(nn.Module):\n",
        "    def __init__(self, filter_sizes, num_filters, num_classes):\n",
        "        super(CNN_NLP, self).__init__()\n",
        "        self.embed_dim=300\n",
        "        self.conv1d_list = nn.ModuleList([nn.Conv1d(in_channels=self.embed_dim, out_channels=num_filters[i],kernel_size=filter_sizes[i]) \n",
        "                                          for i in range(len(filter_sizes))])\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x_embed): #m,len,300\n",
        "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out). A list \n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "        # Max pooling. Output shape: (b, num_filters[i], 1), a list\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in x_conv_list]\n",
        "        # Concatenate x_pool_list to feed the fully connected layer.\n",
        "        # Output shape: (b, sum(num_filters))\n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list], dim=1)\n",
        "        # Compute logits. Output shape: (b, n_classes)\n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "        return logits\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "filter_sizes=[2,3]\n",
        "num_filters=[20,30]\n",
        "num_classes=5\n",
        "model=CNN_NLP(filter_sizes,num_filters,num_classes).to(device)"
      ],
      "metadata": {
        "id": "xgto75BL8GLW"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader,model,batchsize_grad,epochs,optimizer,criterion, num_batch, val_dataloader,len_val):\n",
        "    acc_steps = 100\n",
        "    model.train()\n",
        "    accumulating_batch_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            model.train()\n",
        "            inputs=batch[0].to(device) #[m,len,300]\n",
        "            logits = model(inputs) #[m,5] \n",
        "            targets=batch[1].to(device) #m\n",
        "            loss = criterion(logits,targets)\n",
        "            loss.backward() #The gradients are computed when we call loss. backward() and are stored by PyTorch until we call optimizer.\n",
        "            if accumulating_batch_count % batchsize_grad == 0: #when accumulated batch=16, we do optimizer after 16 batches of gradients are accumulated\n",
        "                optimizer.step()\n",
        "                #scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "            accumulating_batch_count += 1\n",
        "            # for evaluate the model after certain batches\n",
        "            if accumulating_batch_count % len(train_dataloader)==0:\n",
        "                model.eval()\n",
        "                accuracy=0\n",
        "                for i, batch in enumerate(val_dataloader):\n",
        "                    inputs=batch[0].to(device) #[m,512]\n",
        "                    with torch.no_grad():\n",
        "                      logits = model(inputs) #[m,5]\n",
        "                    softmaxed=torch.softmax(logits,-1) #[m,5]\n",
        "                    predict_label=torch.argmax(softmaxed,-1).to('cpu')\n",
        "                    targets=batch[1].to('cpu') #m\n",
        "                    from sklearn.metrics import accuracy_score\n",
        "                    accuracy+=accuracy_score(targets,predict_label)*batch[0].shape[0]\n",
        "        #print the loss and accuracy of the validation set after each epoch\n",
        "        print (loss.item(),accuracy/len_val)\n",
        "        #save the best model\n",
        "        if accuracy/len_val>0.9: path=\"best_model.pt\"; torch.save(model.state_dict(), path) \n"
      ],
      "metadata": {
        "id": "3yjUphuXGzfa"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=CNN_NLP(filter_sizes,num_filters,num_classes).to(device)\n",
        "batch_size=1\n",
        "epochs=40 #simple model uses more epochs\n",
        "lr=0.01 #simple models uses larger lr\n",
        "#num_batch=round(len(train_data)/batch_size)-1\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(test_data+val_data, batch_size=batch_size, shuffle=True)\n",
        "len_val=len(val_dataloader)\n",
        "batchsize_grad=20\n",
        "criterion=torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
        "train(train_dataloader,model,batchsize_grad,epochs,optimizer,criterion, num_batch,val_dataloader, len_val)"
      ],
      "metadata": {
        "id": "IMC5A6xsHUC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model test"
      ],
      "metadata": {
        "id": "16OupraSCIe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"best_model.pt\" \n",
        "model.load_state_dict(torch.load(path))\n",
        "\n",
        "val_dataloader = DataLoader(data, batch_size=1, shuffle=True)\n",
        "model.eval()\n",
        "accuracy=0\n",
        "for i, batch in enumerate(val_dataloader):\n",
        "    inputs=batch[0].to(device) #[m,512]\n",
        "    with torch.no_grad():\n",
        "      logits = model(inputs) #[m,5]\n",
        "    softmaxed=torch.softmax(logits,-1) #[m,5]\n",
        "    predict_label=torch.argmax(softmaxed,-1).to('cpu')\n",
        "    targets=batch[1].to('cpu') #m\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    accuracy+=accuracy_score(targets,predict_label)*batch[0].shape[0]\n",
        "print (accuracy/len(val_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOIdXQLgCJ-_",
        "outputId": "59bf8cec-280a-4ed3-9f03-86db187bb8a3"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9819354838709677\n"
          ]
        }
      ]
    }
  ]
}