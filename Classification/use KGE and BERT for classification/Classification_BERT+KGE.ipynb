{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.0 Install and import library"
      ],
      "metadata": {
        "id": "ag5kres6XLaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install fire\n"
      ],
      "metadata": {
        "id": "wrDXqD7VBhNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BM_M8m1jBOYo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define tokenizer, dataloader, bertmodel"
      ],
      "metadata": {
        "id": "labwFfghciwk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "BZNLwqHRBOYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f93bb2-fee8-432f-c68c-7ff829ebeee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bertmodel=BertModel.from_pretrained('bert-base-cased').to(device)\n",
        "for name, param in bertmodel.named_parameters():\n",
        "  param.requires_grad=True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.0 Prepare dataset into the train, test"
      ],
      "metadata": {
        "id": "1_6ueCTUWB-U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq0ZM7y6BOYr"
      },
      "source": [
        "## Read original texts, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sWtkrJZ4BOYv"
      },
      "outputs": [],
      "source": [
        "# Load original texts and labels\n",
        "df=pd.read_json('All_Beauty_5.json', lines=True)\n",
        "texts = [' '.join([str(i),str(j)]) for i,j in zip(df['reviewText'],df['summary'])]\n",
        "labels = [i for i in df['overall']]\n",
        "\n",
        "# Classes are imbalanced, so we need to remove some samples. \n",
        "new_texts=[]\n",
        "new_labels=[]\n",
        "N_5=0\n",
        "for i in range(len(labels)):\n",
        "  if labels[i]==5:\n",
        "    N_5+=1\n",
        "    if N_5<156:new_texts.append(texts[i]);new_labels.append(labels[i])\n",
        "  else: new_texts.append(texts[i]);new_labels.append(labels[i])\n",
        "\n",
        "# Assign new texts and labels as our dataset\n",
        "texts=new_texts\n",
        "labels=new_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We want to get several things before injecting KG_embeddings.\n",
        "* `list_entities`: for each input, it contains a list of entities that appear in the list. like `[[I love], [you]]`\n",
        "* `entities_list_inputs_bert_ids`: for each input, it contains a list of bert_ids of entities. Like `[[3599,129],[9982]]`\n",
        "* `entities_token_indexes`: for each input, it contains a list of position ids where an entity appears in the input. Like `[[8,9],[11]]`. This is obtained by comparing the bert_ids of entities (above) and bert_ids of the input.\n",
        "* `list_kg_embeddings`: for each input, it contains a list of embeddings of the entities trained from KG. Like `[[emb_3599, emb_129],[emb_9982]]`\n",
        "* So for each input, we want to combine the `list_kg_embeddings` with the original embeddings."
      ],
      "metadata": {
        "id": "rxcL6QZAeYdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create bert_tokenizer ids of the entities\n",
        "KG=torch.load('/content/data_for_KGE_training.pt')\n",
        "entities=KG['entities']\n",
        "#list: [1533, m]\n",
        "bert_ids_of_entities=[]\n",
        "for i in entities:\n",
        "  bert_ids_of_entities.append(tokenizer(i, return_tensors=\"pt\")['input_ids'][0:,1:-1].squeeze(0).tolist())"
      ],
      "metadata": {
        "id": "8fRuOttrlVRQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for getting the following three lists\n",
        "list_entities=[]\n",
        "entities_list_inputs_bert_ids=[]\n",
        "entities_token_indexes=[]\n",
        "\n",
        "for i in texts:\n",
        "  input_id=tokenizer(i,padding=False, max_length=512, truncation=True, return_tensors=\"pt\")['input_ids'].squeeze(0).to(device) #[seq]\n",
        "  a=input_id.tolist()\n",
        "  entities_token_index=[]\n",
        "  entities_list_inputs=[]\n",
        "  list_entity=[]\n",
        "\n",
        "  for j in range (len(bert_ids_of_entities)):\n",
        "    b=bert_ids_of_entities[j]\n",
        "    if (' '+(' ').join([str(i) for i in b])+' ') in ((' ').join([str(i) for i in a])):\n",
        "      entity_id=[0]*len(b)\n",
        "      entity_id[0]=a.index(b[0])\n",
        "      for k in range(1,len(b)):\n",
        "        entity_id[k]=entity_id[k-1]+1\n",
        "      entities_token_index.append(entity_id)\n",
        "      entities_list_inputs.append(b)\n",
        "      list_entity.append(entities[j])\n",
        "  entities_token_indexes.append(entities_token_index)\n",
        "  entities_list_inputs_bert_ids.append(entities_list_inputs)\n",
        "  list_entities.append(list_entity)"
      ],
      "metadata": {
        "id": "LdnW8Lfdu_8L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for getting list_kg_embeddings\n",
        "embeddings=torch.load('/content/KG_embeddings.pt')\n",
        "list_kg_embeddings=[]\n",
        "for i in list_entities:\n",
        "  list_kg_embedding=[]\n",
        "  for j in i:\n",
        "    list_kg_embedding.append(embeddings[j])\n",
        "  list_kg_embeddings.append(list_kg_embedding)"
      ],
      "metadata": {
        "id": "pEM8e1XvFnCN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrate all things into data list"
      ],
      "metadata": {
        "id": "2STDYLykmOQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map labels\n",
        "target_names = list(set(labels))\n",
        "label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
        "print(label2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLUEwT_MnMjf",
        "outputId": "b55b26f6-7436-4889-faba-7321ea862180"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create data list, which can be used for dataloader. \n",
        "labels_tensor=[torch.tensor(label2idx[i]) for i in labels]\n",
        "data=[[i,j,k,l,m,n]for i,j,k,l,m,n in zip (texts,labels_tensor,list_entities,entities_list_inputs_bert_ids,entities_token_indexes,list_kg_embeddings)]\n",
        "#divide into train, test\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(data, test_size=0.13, random_state=0)"
      ],
      "metadata": {
        "id": "6zUg6hotRy_E"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpA2EWnsBOYz"
      },
      "source": [
        "# 3.0 Create the model and define the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create our model using Class"
      ],
      "metadata": {
        "id": "60PNZAimWzNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model0 "
      ],
      "metadata": {
        "id": "hzrJfTzqfc4v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "KWXcNn_NBOY0"
      },
      "outputs": [],
      "source": [
        "# model0:insert kg_embedding after transformer, the pass through another transformer\n",
        "class Bert_KG0(torch.nn.Module):\n",
        "    def __init__(self,bertmodel): \n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(in_features=150, out_features=768, bias=True)\n",
        "        self.bertmodel=bertmodel\n",
        "        self.linear2 = torch.nn.Linear(in_features=768, out_features=5, bias=True)\n",
        "\n",
        "    def forward(self, batch): #input_ids,[m,768]\n",
        "        input_id=tokenizer(batch[0], return_tensors=\"pt\")['input_ids'].to(device) #[seq]\n",
        "        logits0 = bertmodel(input_id[:,:512])[0].squeeze(0) #[seq 768]\n",
        "        logits=logits0.clone()\n",
        "        for j in range(len(batch[-1])):\n",
        "          embedding_to_add=self.linear1(torch.tensor(batch[-1][j]).float().to(device))\n",
        "          for k in range(len(batch[-2][j])):\n",
        "            logits[batch[-2][j][k]]=(0.7*logits[batch[-2][j][k]]+0.3*embedding_to_add)\n",
        "        logits=bertmodel.encoder(logits.unsqueeze(0))[0].squeeze(0)\n",
        "        CLS=logits[0]\n",
        "        logits=self.linear2(CLS) # [m,5]\n",
        "        return logits #(m,5)\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(0)\n",
        "model=Bert_KG0(bertmodel).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model1"
      ],
      "metadata": {
        "id": "y1mjo3Qtfg0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model1: just bert with linear layer\n",
        "class Bert_KG1(torch.nn.Module):\n",
        "    def __init__(self,bertmodel): \n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(in_features=150, out_features=768, bias=True)\n",
        "        self.bertmodel=bertmodel\n",
        "        self.linear2 = torch.nn.Linear(in_features=768, out_features=5, bias=True)\n",
        "\n",
        "    def forward(self, batch): #input_ids,[m,768]\n",
        "        input_id=tokenizer(batch[0], return_tensors=\"pt\")['input_ids'].to(device) #[seq]\n",
        "        logits0 = bertmodel(input_id[:,:512])[0].squeeze(0) #[seq 768]\n",
        "        CLS=logits0[0]\n",
        "        logits=self.linear2(CLS) # [m,5]\n",
        "        return logits #(m,5)\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(0)\n",
        "model=Bert_KG1(bertmodel).to(device)"
      ],
      "metadata": {
        "id": "LKZK76PnVZFa"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model 2"
      ],
      "metadata": {
        "id": "AL1mKF8ffibd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model2:insert the kg_embedding into before transformer\n",
        "class Bert_KG2(torch.nn.Module):\n",
        "    def __init__(self,bertmodel): \n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(in_features=150, out_features=768, bias=True)\n",
        "        self.bertmodel=bertmodel\n",
        "        self.linear2 = torch.nn.Linear(in_features=768, out_features=5, bias=True)\n",
        "\n",
        "    def forward(self, batch): #input_ids,[m,768]\n",
        "        input_id=tokenizer(batch[0], return_tensors=\"pt\")['input_ids'].to(device) #[seq]\n",
        "        logits0 = bertmodel.embeddings(input_id[:,:512])[0].squeeze(0) #[seq 768]\n",
        "        logits=logits0.clone()\n",
        "        for j in range(len(batch[-1])):\n",
        "          embedding_to_add=self.linear1(torch.tensor(batch[-1][j]).float().to(device))\n",
        "          for k in range(len(batch[-2][j])):\n",
        "            logits[batch[-2][j][k]]=(0.7*logits[batch[-2][j][k]]+0.3*embedding_to_add)\n",
        "        logits=bertmodel.encoder(logits.unsqueeze(0))[0].squeeze(0)\n",
        "        CLS=logits[0]\n",
        "        logits=self.linear2(CLS) # [m,5]\n",
        "        return logits #(m,5)\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(0)\n",
        "model=Bert_KG2(bertmodel).to(device)"
      ],
      "metadata": {
        "id": "IJAq_j1BaTQB"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the training process"
      ],
      "metadata": {
        "id": "fRr7njtpyIMe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "kxqSVq1zBOY2"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion, val_dataloader,len_val):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Training epoch {epoch+1}\")\n",
        "        model.train()\n",
        "        loss_accumulate=0\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            logits = model(batch) #m,5\n",
        "            targets=batch[1].to(device) #m\n",
        "            loss = criterion(logits.unsqueeze(0),targets)/batchsize_grad\n",
        "            loss_accumulate+=loss\n",
        "            loss.backward() #The gradients are computed when we call loss. backward() and are stored by PyTorch until we call optimizer.\n",
        "            \n",
        "            if (i+1) % batchsize_grad == 0: #when accumulated batch=16, we do optimizer after 16 batches of gradients are accumulated\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "                if (i+1) % 100 == 0: print (i+1, loss_accumulate)\n",
        "                loss_accumulate=0\n",
        "                \n",
        "        #for evaluate the model after an epoch\n",
        "        model.eval()\n",
        "        accuracy=0\n",
        "        for i, batch in enumerate(val_dataloader):\n",
        "            with torch.no_grad():\n",
        "              logits = model(batch) #[m,5]\n",
        "            softmaxed=torch.softmax(logits,-1) #[m,5]\n",
        "            predict_label=torch.argmax(softmaxed,-1).to('cpu')\n",
        "            targets=batch[1].to('cpu') #m\n",
        "            from sklearn.metrics import accuracy_score\n",
        "            accuracy+=accuracy_score([targets[0]],[predict_label])\n",
        "        print (\"accuracy\",accuracy/len_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.0 Start training:remember to save the model"
      ],
      "metadata": {
        "id": "RjTufCUkyKtv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7cq42T2BOY3"
      },
      "outputs": [],
      "source": [
        "batch_size=1\n",
        "batchsize_grad=10\n",
        "epochs=12\n",
        "lr=5e-5\n",
        "torch.manual_seed(0)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
        "len_val=len(val_dataloader)\n",
        "criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor([1, 1, 1, 1, 1],dtype=torch.float).to(device))\n",
        "torch.manual_seed(0)\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "torch.manual_seed(0)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*epochs/batchsize_grad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion,val_dataloader, len_val)"
      ],
      "metadata": {
        "id": "_pCHaRHltfx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.0 Result:\n",
        "\n",
        "Refer to the 3 files. But the general result is as follows:\n",
        "* KG_embeddings after transformer can get the best result (Model 1), the highest accuracy reach 95.5% at epoch 8. Time: 11m 23s\n",
        "* KG_embeddings before transformer reach 93.1% at epoch 4. Time: 5m 40s\n",
        "* Bert+linear reach 91.1% at epoch 4. Time: 5m 17s\n",
        "\n",
        "# 6.0 Conclusion:\n",
        "KG_embedding is useful, because it fully captures the correlation between different entities in the entire corpus. "
      ],
      "metadata": {
        "id": "BMtUiRJcsFSz"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Classification: BERT+KGE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}