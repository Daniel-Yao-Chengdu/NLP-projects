{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text collection and processing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XxDneBeiYuLu",
        "eigQpMiiCGY_",
        "BwhgoGzaF4Ep",
        "mOrI8mHAIzpO",
        "mVtEe_0sKHfS",
        "puO1yc4oRMSe"
      ],
      "authorship_tag": "ABX9TyO+NeS00nPLanlABeQEYVyD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniel-Yao-Chengdu/NLP-project/blob/master/text_collection_and_processing_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# text crawling\n",
        "* slate3k: for 1 column\n",
        "* pytesseract: for 2 column"
      ],
      "metadata": {
        "id": "XxDneBeiYuLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import slate3k as slate\n",
        "import os\n",
        "\n",
        "texts=[]\n",
        "folder = 'C:\\\\Users\\\\smrya\\Desktop\\\\博士\\\\Journal article\\\\Journal article 2\\\\Data\\\\4. books\\\\'\n",
        "N=0\n",
        "for filename in os.listdir(folder):\n",
        "    N+=1; print (N)\n",
        "    path = os.path.join(folder, filename)\n",
        "    with open(path,'rb') as f:\n",
        "        text = slate.PDF(f)\n",
        "        text=' '.join(text) #join the text into a whole string\n",
        "    texts.append(text) #return [m,text]"
      ],
      "metadata": {
        "id": "RMKdvxoGYvac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "folder = 'C:\\\\Users\\\\smrya\\Desktop\\\\博士\\\\Journal article\\\\Journal article 2\\\\Data\\\\6. reports\\\\'\n",
        "#for setting the path\n",
        "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
        "poppler_path=r'C:\\Users\\smrya\\anaconda3\\Lib\\site-packages\\poppler-0.68.0\\poppler-0.68.0\\bin'\n",
        "\n",
        "texts=[]\n",
        "N=0\n",
        "for filename in os.listdir(folder):\n",
        "    N+=1;print (N)\n",
        "    path = os.path.join(folder, filename)\n",
        "    images = convert_from_path(path,poppler_path=poppler_path)\n",
        "    text = ''\n",
        "    for i in range(len(images)):  \n",
        "        page_content = pytesseract.image_to_string(images[i])\n",
        "        text = text + ' ' + page_content\n",
        "    texts.append(text) #return [m,text]]"
      ],
      "metadata": {
        "id": "j0Hw6jIIbgPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sentence segmentation: mysentences\n",
        "* This is the code that I wrote for turning an unstructured document (messy words) into a list sentences. \n",
        "* This is stored in local files as \"mysentences\" and can be revised at any time.\n",
        "* we can also use other libraries, like nltk for sentence segmentation"
      ],
      "metadata": {
        "id": "JaGK2x4wWT8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sentence cleaning, tokenization, lemmatization, etc."
      ],
      "metadata": {
        "id": "DrLBlsPVcIGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is for practicing different kinds of NLP libraries for word and sentence tokenization, sentence segmentation, sentence lemmatization...\n",
        "* BERT\n",
        "* GPT\n",
        "* Other tokenizers in huggingface\n",
        "* NLTK\n",
        "* Spacy\n",
        "* Gensim\n",
        "* Corenlp\n",
        "* Chinese: 哈工大PYLTP、中科院计算所NLPIR、清华大学THULAC, Jieba、FoolNLTK、HanLP\n",
        "* myRaw2lemmatized\n",
        "\n",
        "Common steps for text processing: choose what you want\n",
        " 1. remove blanks rows if any; \n",
        " 2. change all the text in to lower case; \n",
        " 3. word tokenization;\n",
        " 4. remove stop words; \n",
        " 5. remove non-alpha text; \n",
        " 6. word lemmatization/stemming"
      ],
      "metadata": {
        "id": "6Q_L1wd-_VkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## myRaw2lemmatized: an entire cycle\n",
        "* This is for transforming a list of raw sentences [m,1] to cleaned and lemmatized [m, words]\n",
        "* Process: \n",
        "  1. lower case; \n",
        "  2. word_tokenization; NLTK \n",
        "  3. remove stop words; \n",
        "  4. remove non-alpha; \n",
        "  5. word lemmatization\n",
        "* More suitable for non-BERT task\n",
        "* The result can be used for creating tf-idf/wordcount or word-2-vec static embedding\n"
      ],
      "metadata": {
        "id": "-7m-3fHwXfzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Huggingface-BERT, GPT and others"
      ],
      "metadata": {
        "id": "eigQpMiiCGY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "ykIpiZ94DJHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='bert-base-uncased' #change to the model you want, like 'nreimers/TinyBERT_L-4_H-312_v2'\n",
        "tokenizer=AutoTokenizer.from_pretrained(path, return_tensors='pt', max_length=None, padding=False, truncation=False)"
      ],
      "metadata": {
        "id": "BRoVk-sZCyOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hanlp\n",
        "* for dealing with Chinese\n",
        "* more for traditional machine learning"
      ],
      "metadata": {
        "id": "BwhgoGzaF4Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyhanlp\n",
        "from pyhanlp import *"
      ],
      "metadata": {
        "id": "j9cc-CaWF6qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"我爱自然语言处理, 不爱你, 你说你气不气\"\n",
        "#sentence=\"I love you, you are the most important one, incredibly\"\n",
        "words=HanLP.segment(sentence)\n",
        "for word in words:\n",
        "  print (word.word, word.nature)"
      ],
      "metadata": {
        "id": "uo0AjVT1GQLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jieba\n",
        "* Often used\n",
        "* 精确模式，试图将句子最精确地切开，适合文本分析，调用形式是jieba.lcut(string)；\n",
        "* 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义，冗余最大，调用形式是jieba.lcut(string,cut_all=True)；\n",
        "* 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词,jieba.lcut_for_search(string)。"
      ],
      "metadata": {
        "id": "mOrI8mHAIzpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jieba\n",
        "import jieba"
      ],
      "metadata": {
        "id": "19CqC9XiIFPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"我爱自然语言处理, 不爱你, 你说你气不气。我确实很气\"\n",
        "words=jieba.cut(sentence)\n",
        "print (\" \".join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI03kfQLJmlb",
        "outputId": "8cf7bf66-5d1a-4d7d-9e06-491c9d95167b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我 爱 自然语言 处理 ,   不爱 你 ,   你 说 你 气不气 。 我 确实 很 气\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK\n",
        "* word_tokenize\n",
        "* sent_tokenize"
      ],
      "metadata": {
        "id": "mVtEe_0sKHfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NLTK"
      ],
      "metadata": {
        "id": "KZaofAexJswb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentence = \"I love you, but also love NLP incredibly\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "sentences=nltk.sent_tokenize('I love you,     but also love NLP incredibly. I love you??   ')"
      ],
      "metadata": {
        "id": "jPzoAsgrKKjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gensim\n",
        "* simple_preprocess\n",
        "* tokenizer"
      ],
      "metadata": {
        "id": "q_o0l87kMiZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUQKfKE1Mlja",
        "outputId": "449ba95d-b5f2-4172-ae1a-2536b4d33c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "sentence = \"I love you, but also love NLP incredibly, hi\"\n",
        "gensim.utils.simple_preprocess(sentence, min_len=1, deacc=False, max_len=15)"
      ],
      "metadata": {
        "id": "xQfhkfU6Mntd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=gensim.utils.tokenize(sentence, lowercase=False, deacc=False, errors='strict', to_lower=False, lower=False)\n",
        "print ([i for i in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJmLxLaZMr3g",
        "outputId": "1c66344d-83f1-4080-a318-b0060721059e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'you', 'but', 'also', 'love', 'NLP', 'incredibly', 'hi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CoreNLP"
      ],
      "metadata": {
        "id": "2VH4WU7BhiZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanfordcorenlp\n",
        "!wget http://nlp.stanford.edu/software/stanford-corenlp-latest.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zP9MH6jhnWN",
        "outputId": "545d8785-0ff9-434c-bc04-221205f5fdb3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stanfordcorenlp in /usr/local/lib/python3.7/dist-packages (3.9.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanfordcorenlp) (2.23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stanfordcorenlp) (5.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (1.24.3)\n",
            "--2022-06-08 23:15:21--  http://nlp.stanford.edu/software/stanford-corenlp-latest.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-corenlp-latest.zip [following]\n",
            "--2022-06-08 23:15:22--  https://nlp.stanford.edu/software/stanford-corenlp-latest.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-latest.zip [following]\n",
            "--2022-06-08 23:15:22--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-latest.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 505207915 (482M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-latest.zip.2’\n",
            "\n",
            "stanford-corenlp-la 100%[===================>] 481.80M  5.12MB/s    in 91s     \n",
            "\n",
            "2022-06-08 23:16:53 (5.32 MB/s) - ‘stanford-corenlp-latest.zip.2’ saved [505207915/505207915]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'stanford-corenlp-latest.zip' -d '/content'"
      ],
      "metadata": {
        "id": "hNJ122OMmY3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change the directory\n",
        "import os\n",
        "os.chdir('/content/stanford-corenlp-4.4.0')\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CeDsrj-mC6p",
        "outputId": "156c893d-eaea-4353-d3e2-0278b72b2b7b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stanford-corenlp-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stanfordcorenlp import StanfordCoreNLP\n",
        " \n",
        "nlp = StanfordCoreNLP(r'/content/stanford-corenlp-4.4.0', lang='en')\n",
        " \n",
        "sentence = 'you are the best boy'\n",
        "print (nlp.word_tokenize(sentence))\n",
        "print (nlp.pos_tag(sentence))\n",
        "print (nlp.ner(sentence))\n",
        "print (nlp.parse(sentence))\n",
        "print (nlp.dependency_parse(sentence))"
      ],
      "metadata": {
        "id": "BBbubvsSh_A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'you are the best boy, incredibly, 111, unprecedentedly'\n",
        "print (nlp.word_tokenize(sentence))"
      ],
      "metadata": {
        "id": "wL3s__fWn1Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pyltp"
      ],
      "metadata": {
        "id": "puO1yc4oRMSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyltp"
      ],
      "metadata": {
        "id": "lMzCV8cXNWKL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}