{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook:\n",
        "* have not used pretrained embeddings\n",
        "* the best hidden size is 90, with accuracy around 88%"
      ],
      "metadata": {
        "id": "rR8EoS_Nrlk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.0 Install and import library"
      ],
      "metadata": {
        "id": "ag5kres6XLaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install fire"
      ],
      "metadata": {
        "id": "wrDXqD7VBhNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6059a85-e7ed-4781-ffd4-a16294468b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=987db4f4339a2ad6f964f0d2c1be17bdf8678d4dd86f57a219a8dc995b004bd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM_M8m1jBOYo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.0 Prepare dataset into the train, val, test: [text_tensor, label_tensor]"
      ],
      "metadata": {
        "id": "1_6ueCTUWB-U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq0ZM7y6BOYr"
      },
      "source": [
        "## Read original texts, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWtkrJZ4BOYv"
      },
      "outputs": [],
      "source": [
        "# Load original texts and labels\n",
        "df=pd.read_json('All_Beauty_5.json', lines=True)\n",
        "texts = [' '.join([str(i),str(j)]) for i,j in zip(df['reviewText'],df['summary'])]\n",
        "labels = [i for i in df['overall']]\n",
        "\n",
        "# Classes are imbalanced, so we need to remove some samples. \n",
        "new_texts=[]\n",
        "new_labels=[]\n",
        "N_5=0\n",
        "for i in range(len(labels)):\n",
        "  if labels[i]==5:\n",
        "    N_5+=1\n",
        "    if N_5<156:new_texts.append(texts[i]);new_labels.append(labels[i])\n",
        "  else: new_texts.append(texts[i]);new_labels.append(labels[i])\n",
        "\n",
        "# Assign new texts and labels as our dataset\n",
        "texts=new_texts\n",
        "labels=new_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=[]\n",
        "for i in texts:\n",
        "  vocab+=i.lower().split(' ')\n",
        "vocab=set(vocab)\n",
        "vocab=list(vocab)"
      ],
      "metadata": {
        "id": "WYWXS-ibCjAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_index=[]\n",
        "for i in range(len(texts)):\n",
        "  texts_index.append([vocab.index(j) for j in texts[i].lower().split(' ')])"
      ],
      "metadata": {
        "id": "YaRTSMrmC8SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOjl_KpRBOYw"
      },
      "source": [
        "## Create a list: `[data_tensor, label_tensor]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Go81HZpBOYw",
        "outputId": "0b838ad8-5a0e-4fbd-91a4-91f8154a65d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n"
          ]
        }
      ],
      "source": [
        "#Next, we need to determine the number of labels in our data. We'll map each of these labels to an index.\n",
        "target_names = list(set(labels))\n",
        "label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
        "print(label2idx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create data list, which can be used for dataloader. \n",
        "labels=[label2idx[i] for i in labels]\n",
        "data=[[i, j]for i,j in zip (texts_index,labels)]\n",
        "#path=\"data.pt\"\n",
        "#data=torch.load(path)"
      ],
      "metadata": {
        "id": "6zUg6hotRy_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5IhcOMgElI8",
        "outputId": "b6ce6dcc-4333-4a4f-c771-ee2eed171fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4976, 4408, 122, 3952, 1900, 2628], 4]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divide into train, val, test"
      ],
      "metadata": {
        "id": "2JMMKcHSx9Vt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1IHveOkBOYz"
      },
      "outputs": [],
      "source": [
        "#divide into train, val, test\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(data, test_size=0.13, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6FvJ_tL0bcl",
        "outputId": "af850c3e-4131-4b2c-a3f4-f9c897b4087a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5363"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpA2EWnsBOYz"
      },
      "source": [
        "# 3.0 Create the model and divide the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create our model using Class"
      ],
      "metadata": {
        "id": "60PNZAimWzNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWXcNn_NBOY0"
      },
      "outputs": [],
      "source": [
        "#create a BERT + Linear model\n",
        "class NNmodel(torch.nn.Module):\n",
        "    def __init__(self,features): \n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=5363, embedding_dim=features)\n",
        "        self.linear1=torch.nn.Linear(in_features=features,out_features=features,bias=True)\n",
        "        self.linear2=torch.nn.Linear(in_features=features,out_features=features,bias=True)\n",
        "        self.linear3=torch.nn.Linear(in_features=features,out_features=features,bias=True)\n",
        "        self.linear4=torch.nn.Linear(in_features=features,out_features=features,bias=True)\n",
        "        self.linear5=torch.nn.Linear(in_features=features,out_features=5,bias=True)\n",
        "        self.LSTM = torch.nn.LSTM(input_size=27, hidden_size=27, batch_first=True) \n",
        "\n",
        "    def forward(self, x): #input_ids,[seq] \n",
        "        x=self.embedding(x) #seq,20\n",
        "        x=self.linear1(x) #seq,20\n",
        "        x=torch.relu(x)  #seq,20\n",
        "        x=self.linear2(x) #seq,20\n",
        "        x=torch.relu(x) #seq,20\n",
        "        x=self.linear3(x) #seq,20\n",
        "        x=torch.relu(x) #seq,20\n",
        "        #x=self.LSTM(x)[0][-1] # xï¼š(batch_size, seq_length, hidden_size)\n",
        "        x=torch.mean(x,0) #1,20\n",
        "        #x=torch.sum(x)\n",
        "        #other means\n",
        "        x=self.linear4(x) #1,20\n",
        "        x=torch.relu(x) #1,20\n",
        "        x=self.linear5(x)\n",
        "        return x #(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a BERT + Linear model\n",
        "class LSTMLinear(torch.nn.Module):\n",
        "    def __init__(self,features): \n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=5363, embedding_dim=features)\n",
        "        self.LSTM = torch.nn.LSTM(input_size=features, hidden_size=features, batch_first=True) \n",
        "        self.linear=torch.nn.Linear(in_features=features,out_features=5,bias=True)\n",
        "\n",
        "    def forward(self, x): #input_ids,[seq]\n",
        "        x=self.embedding(x)\n",
        "        x,(hidden_state,cell_state) = self.LSTM(x) # xï¼š(batch_size, seq_length, hidden_size)\n",
        "        x=self.linear(x[-1])\n",
        "        return x #(5)"
      ],
      "metadata": {
        "id": "qU6-wMREFxBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the training process"
      ],
      "metadata": {
        "id": "fRr7njtpyIMe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxqSVq1zBOY2"
      },
      "outputs": [],
      "source": [
        "#train our model\n",
        "def train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion, num_batch, val_dataloader,len_val):\n",
        "\n",
        "    acc_steps = 100\n",
        "    model.train()\n",
        "\n",
        "    accumulating_batch_count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            inputs=torch.tensor(batch[0]).to(device) #[m,768]\n",
        "            logits = model(inputs).unsqueeze(0) #[m,5] \n",
        "            targets=batch[1].to(device) #m\n",
        "            loss = criterion(logits,targets)/batchsize_grad\n",
        "            loss.backward() #The gradients are computed when we call loss. backward() and are stored by PyTorch until we call optimizer.\n",
        "            \n",
        "            accumulating_batch_count += 1\n",
        "            if accumulating_batch_count % batchsize_grad == 0: #when accumulated batch=16, we do optimizer after 16 batches of gradients are accumulated\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "                #print (round(loss.item(),4))\n",
        "        \n",
        "        accuracy=0\n",
        "        model.eval()\n",
        "        for i, batch in enumerate(val_dataloader):\n",
        "            inputs=torch.tensor(batch[0]).to(device) #[m,768]\n",
        "            with torch.no_grad():\n",
        "              logits = model(inputs).unsqueeze(0) #[m,5] \n",
        "            softmaxed=torch.softmax(logits,-1) #[m,5]\n",
        "            predict_label=torch.argmax(softmaxed,-1).to('cpu')\n",
        "            #print (predict_label)\n",
        "            targets=batch[1].to('cpu') #m\n",
        "            from sklearn.metrics import accuracy_score\n",
        "            accuracy+=accuracy_score(targets,predict_label)\n",
        "        print (accuracy/len(val_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.0 Start training:remember to save the model"
      ],
      "metadata": {
        "id": "RjTufCUkyKtv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7cq42T2BOY3"
      },
      "outputs": [],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(0)\n",
        "model=NNmodel(27).to(device)\n",
        "\n",
        "batch_size=1\n",
        "batchsize_grad=20\n",
        "epochs=15 #simple model uses more epochs\n",
        "lr=0.008 #simple models uses larger lr\n",
        "len_val=len(test_data)\n",
        "num_batch=round(len(train_data)/batch_size)-1\n",
        "torch.manual_seed(0)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "criterion=torch.nn.CrossEntropyLoss()\n",
        "torch.manual_seed(0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=epochs*len(train_data)/batchsize_grad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataloader,model,batchsize_grad,epochs,scheduler,optimizer,criterion, num_batch,val_dataloader, len_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mtxZ3txWdgd",
        "outputId": "63984b83-a16a-44b1-bc3d-ed8e732e2f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 0\n",
            "0.44554455445544555\n",
            "Training epoch 1\n",
            "0.44554455445544555\n",
            "Training epoch 2\n",
            "0.6732673267326733\n",
            "Training epoch 3\n",
            "0.7821782178217822\n",
            "Training epoch 4\n",
            "0.8415841584158416\n",
            "Training epoch 5\n",
            "0.8415841584158416\n",
            "Training epoch 6\n",
            "0.8910891089108911\n",
            "Training epoch 7\n",
            "0.8910891089108911\n",
            "Training epoch 8\n",
            "0.8811881188118812\n",
            "Training epoch 9\n",
            "0.8811881188118812\n",
            "Training epoch 10\n",
            "0.8811881188118812\n",
            "Training epoch 11\n",
            "0.8811881188118812\n",
            "Training epoch 12\n",
            "0.8811881188118812\n",
            "Training epoch 13\n",
            "0.8811881188118812\n",
            "Training epoch 14\n",
            "0.8811881188118812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save our model parameters\n",
        "N+=1\n",
        "path=\"state_dict_model_\" + str(N) + \".pt\"\n",
        "torch.save(model.state_dict(), path) "
      ],
      "metadata": {
        "id": "5f8oSMAFiaK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.0 Validate our model"
      ],
      "metadata": {
        "id": "eScSw3UtfUpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path=\"state_dict_model_3.pt\" \n",
        "#model.load_state_dict(torch.load(path))\n",
        "\n",
        "input_data=test_data\n",
        "val_dataloader = DataLoader(input_data, batch_size=1, shuffle=False)\n",
        "accuracy=0\n",
        "model.eval()\n",
        "for i, batch in enumerate(val_dataloader):\n",
        "    inputs=torch.tensor(batch[0]).to(device) #[m,768]\n",
        "    with torch.no_grad():\n",
        "      logits = model(inputs).unsqueeze(0) #[m,5] \n",
        "    softmaxed=torch.softmax(logits,-1) #[m,5]\n",
        "    predict_label=torch.argmax(softmaxed,-1).to('cpu')\n",
        "    targets=batch[1].to('cpu') #m\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    accuracy+=accuracy_score(targets,predict_label)\n",
        "print (accuracy/len(input_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V1YNZuQptAY",
        "outputId": "d0f86715-7109-4fc5-ee05-05016adbf687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8811881188118812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.0 Baseline model"
      ],
      "metadata": {
        "id": "WETQoVA8B9if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('lr', LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\"))\n",
        "])\n",
        "\n",
        "parameters = {'lr__C': [0.1, 0.5, 1, 2, 5, 10, 100, 1000]}\n",
        "\n",
        "i=500\n",
        "best_classifier = GridSearchCV(pipeline, parameters, cv=5, verbose=1)\n",
        "best_classifier.fit(texts[0:i], labels[0:i])\n",
        "best_predictions = best_classifier.predict(texts[i:])\n",
        "\n",
        "baseline_accuracy = np.mean(best_predictions == labels[i:])\n",
        "print(\"Baseline accuracy:\", baseline_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCPMZGu7B-7N",
        "outputId": "66945e85-6b01-4795-ca8e-8083d80cd7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Baseline accuracy: 0.6981818181818182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "      # for evaluate the model after certain batches\n",
        "      if accumulating_batch_count % len(train_dataloader)==0:\n",
        "          model.eval()\n",
        "          accuracy=0\n",
        "          for i, batch in enumerate(val_dataloader):\n",
        "              inputs=batch[0].to(device) #[m,512]\n",
        "              with torch.no_grad():\n",
        "                logits = model(inputs) #[m,5]\n",
        "              softmaxed=torch.softmax(logits,-1) #[m,5]\n",
        "              predict_label=torch.argmax(softmaxed,-1).to('cpu')\n",
        "              targets=batch[1].to('cpu') #m\n",
        "              from sklearn.metrics import accuracy_score\n",
        "              accuracy+=accuracy_score(targets,predict_label)*batch[0].shape[0]\n",
        "\n",
        "        #print the loss and accuracy of the validation set after each epoch\n",
        "        print (loss.item(),accuracy/len_val)\n",
        "\n",
        "        #save the best model\n",
        "        if accuracy/len_val>0.82: path=\"best_model.pt\"; torch.save(model.state_dict(), path) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "0o-wIDQGCJj8",
        "outputId": "85ae068e-747c-417e-d0e2-b700f600416c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-240-6b7c2f0306d1>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    print (loss.item(),accuracy/len_val)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "LSTM model without pretrain embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}